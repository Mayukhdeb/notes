<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>


  





<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="page-heading">
        
          <h1>Notes</h1>
          
            <p class="author">Mayukh Deb</p>

            
              <div class="abstract">
                <h5>Abstract</h5>
                <p>
                  This is a place to keep my notes, mostly from the papers I read and find interesting.
                </p>
              </div>
            
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
<div class="container" role="main">
    <div class="posts-list">
      
        
      

      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2023-30-05-mindeye/">
      <h2 class="post-title">MindEye: fMRI-to-Image with Contrastive Learning </h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>May 30, 2023
  
</span>
  </div>
  <div class="post-entry">
    
      <p>What are they doing? The authors found a way to retrieve and reconstruct images from brain signals (FMRI). The 2 aspectes i.e retrieval and reconstruction are handled by 2 different modules.
 For retrieval, they use contrastive learning. For reconstruction, they use diffusion.  Note that retrieval means mapping FMRI signals to a useful embedding space like that of CLIP image emebddings. Which can be used for similarity-search type tasks.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2023-30-05-mindeye/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2023-05-10-brain-inspired-model-training/">
      <h2 class="post-title">BIMT: Brain Inspired Modular Training</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>May 10, 2023
  
</span>
  </div>
  <div class="post-entry">
    
      <p>When compared to artificial neural networks, the human brain is a lot more modular. The authors believe that this is because the loss function of ANN regularizers like weight decay are not dependent on the permutations of neurons on each layer.
The cost of connecting 2 biological neurons which are far apart is much more than when they&rsquo;re closer together. But this is not the case for ANNs.
In order to impose a similar phenomenon to that of brains, the authors propose the following steps:</p>
        <a href="https://mayukhdeb.github.io/notes/post/2023-05-10-brain-inspired-model-training/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2023-04-20-audioclip/">
      <h2 class="post-title">AudioCLIP: Extending CLIP to Image, Text and Audio</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 20, 2023
  
</span>
  </div>
  <div class="post-entry">
    
      <p>They aim to train a tri-modal model which provides a common embedding space for 3 modalities: Images, text and Audio. The trick lies in adding in the 3rd modality (audio)
The repo can be found here.
There are 3 components to this model:
 Image encoder Text encoder Audio Encoder (ESResNeXt)  The image and the text encoder were taken from the original pre-trained CLIP model. The Audio encoder was then first pre-trained on the AudioSet dataset until it achieved a reasonably high accuracy.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2023-04-20-audioclip/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
      </div>
    
    <ul class="pager">
      
      
        <li class="next">
          <a href="https://mayukhdeb.github.io/notes/page/2/">Older &rarr;</a>
        </li>
      
    </ul>
  
</div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2023
      </p>
  </div>
</footer></body>
</html>
