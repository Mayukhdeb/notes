<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes</title>
    <link>https://mayukhdeb.github.io/notes/</link>
    <description>Recent content on Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 24 Oct 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mayukhdeb.github.io/notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>BIMT: Brain Inspired Modular Training</title>
      <link>https://mayukhdeb.github.io/notes/post/2023-05-10-brain-inspired-model-training/</link>
      <pubDate>Wed, 10 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2023-05-10-brain-inspired-model-training/</guid>
      <description>When compared to artificial neural networks, the human brain is a lot more modular. The authors believe that this is because the loss function of ANN regularizers like weight decay are not dependent on the permutations of neurons on each layer.
The cost of connecting 2 biological neurons which are far apart is much more than when they&amp;rsquo;re closer together. But this is not the case for ANNs.
In order to impose a similar phenomenon to that of brains, the authors propose the following steps:</description>
    </item>
    
    <item>
      <title>AudioCLIP: Extending CLIP to Image, Text and Audio</title>
      <link>https://mayukhdeb.github.io/notes/post/2023-04-20-audioclip/</link>
      <pubDate>Thu, 20 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2023-04-20-audioclip/</guid>
      <description>They aim to train a tri-modal model which provides a common embedding space for 3 modalities: Images, text and Audio. The trick lies in adding in the 3rd modality (audio)
The repo can be found here.
There are 3 components to this model:
 Image encoder Text encoder Audio Encoder (ESResNeXt)  The image and the text encoder were taken from the original pre-trained CLIP model. The Audio encoder was then first pre-trained on the AudioSet dataset until it achieved a reasonably high accuracy.</description>
    </item>
    
    <item>
      <title>THINGS: a multimodal dataset for investigating object representations in thehuman brain</title>
      <link>https://mayukhdeb.github.io/notes/post/2023-04-04-things-meg-dataset/</link>
      <pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2023-04-04-things-meg-dataset/</guid>
      <description>It&amp;rsquo;s actually a collection of 3 datasets. All of which can be found here. The images used can be found here.
The 3 datasets are:
 FMRI scans from 3 participants. 8740 images. MEG scans. 4 participants. 22k images. 12k participants. 4.7 million similarity judgements.  The 2 main datasets I&amp;rsquo;m interested in are the MEG scans and the similarity judgements. For the MEG scans, we can use these scripts as boilerplate.</description>
    </item>
    
    <item>
      <title>Open Vocabulary EEG-To-Text Decoding</title>
      <link>https://mayukhdeb.github.io/notes/post/2023-01-25-open-vocab-eeg-text-decoding/</link>
      <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2023-01-25-open-vocab-eeg-text-decoding/</guid>
      <description>The primary problem with most of the existing Brain-Signal to text pipelines is that they have a closed vocabulary.
In this paper, they use pretrained language models for EEG-To-Text decoding and make a zeroshot pilpieline for sentiment classification from EEG. The interesting part is that it can can leverage data from various subjects and sources, i.e huge potential for EEG to text systems with enough data.
Interesting quote from paper:</description>
    </item>
    
    <item>
      <title>Brain2Image: Converting Brain Signals into Images</title>
      <link>https://mayukhdeb.github.io/notes/post/2023-01-25-brain2image-gan/</link>
      <pubDate>Wed, 25 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2023-01-25-brain2image-gan/</guid>
      <description>Can we encode useful visual information about images from the brain&amp;rsquo;s EEG signals?
Yes. Image generation from a brain signal feature vector encoding information about visual classes is the main contribution of this paper
 The authors built an LSTM based generative method which learns a more compact and noise free version of the EEG data and uses it to generate visual stimuli evoking specific brain responses.
EEG contains patterns related to visual content which can be used to generate images which are effective at evoking visual stimuli.</description>
    </item>
    
    <item>
      <title>Neural Taskonomy - using encodings from vision models to understand the human brain</title>
      <link>https://mayukhdeb.github.io/notes/post/2023-01-01-neural-taskonomy/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2023-01-01-neural-taskonomy/</guid>
      <description>Intro CNNs are not too bad at predicting brain activity. But using their intermediate encoding space seems to be not too effective as a way to understand the how the human brain works.
So instead of using a single CNN as a source of visual features, the authors thought that it&amp;rsquo;s a good idea to build encoding models from multiple models which were trained for different vision tasks (segmentation, classification, etc).</description>
    </item>
    
    <item>
      <title>Order From Chaos (Part 1): Diffusion for image synthesis explained in simple words</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-12-25-diffusion/</link>
      <pubDate>Sun, 25 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-12-25-diffusion/</guid>
      <description>Warning: This post is still being written and is not complete, I just uploaded a draft.
Intro If you take and image and iteratively add very small amounts of noise to it, eventually the image would be unrecongnizable to the eye &amp;ndash; Now what if we could undo this process?
Start from noise, and then iteratively remove the noise until you end up with the real image again.
The Forward and backward processes Given an image \(X_0\), we would want to convert it to \(X_T\) by gradually adding noise to \(X_0\) in \(T\) time steps.</description>
    </item>
    
    <item>
      <title>Order From Chaos (Part 2): Diffusion for image synthesis explained in code and a little bit of math</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-12-25-diffusion-code/</link>
      <pubDate>Sun, 25 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-12-25-diffusion-code/</guid>
      <description>Warning: This post is still being written and is not complete, I just uploaded a draft.
This post is basically what I learned while watching this video by DeepFindr.
Diffusion models work by destroying an input gradually until it looks like noise and then recovering the input image from that. The forward process is hardcoded, and the reverse process is trainable.
In the reverse process, the task of the model is to predict the noise that was added in each step to the input image.</description>
    </item>
    
    <item>
      <title>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-12-24-clip-fields/</link>
      <pubDate>Sat, 24 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-12-24-clip-fields/</guid>
      <description>What are they doing? They found a way to help a robot make a &amp;ldquo;map&amp;rdquo; of the world around it in terms of multimodal scene encodings. Then they store these multimodal scene encodings and their respective labels (&amp;ldquo;chair&amp;rdquo;) on a database which is differentiable and is also easily searchable.
How are they doing it?  In order to collect data, they used an RGB-D from which the following data was collected:   RGB-D data bounding boxes from pre-trained object detector  the labels of the bounding boxes are fed into a BERT the bounding boxes on the image are used to crop it and feed each crop into CLIP and store the CLIP encoding   Spatial location (x, y, z)  They used an iphone 13 pro with a LiDAR sensor for depth image sequences.</description>
    </item>
    
    <item>
      <title>CLIPPO: Image and Language understanding from pixels only</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-12-18-clippo-image-and-language-understanding-from-text-only/</link>
      <pubDate>Sun, 18 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-12-18-clippo-image-and-language-understanding-from-text-only/</guid>
      <description>CLIP v/s CLIPPO  CLIP trains 2 seperate image and text encoders, each with it&amp;rsquo;s own preprocessing and embeddings. CLIPPO trains a single transformer where the images are sent in as images and the text is also rendered as a image before shoving it into the forward pass  How do they render text as an image?
Text inputs are rendered on blank images, and are subsequently dealt with entirely as images.</description>
    </item>
    
    <item>
      <title>VPT: Video Pre-Training on Minecraft</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-11-20-clip-fields/</link>
      <pubDate>Sun, 18 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-11-20-clip-fields/</guid>
      <description>Pseudo labelling There are 4 main stages in the data collection stage:
  First, they paid some contractors and obtained 2k hours of labelled minecraft gameplay data. This contained the video frames from within the game and the respective player action (key press, mouse movement). This in total gave them 2k hours of data.
  Then they scrape minecraft videos from youtube and keep only the clean ones. Clean = does not contain the face of the streamer in the corner etc.</description>
    </item>
    
    <item>
      <title>Brain to Text Communication via handwriting</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-11-19-brain-to-text-communication-via-handwriting/</link>
      <pubDate>Sat, 19 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-11-19-brain-to-text-communication-via-handwriting/</guid>
      <description>The original paper can be found here.
The authors of this paper have made a BCI that decodes handwriting movements from neural activity and translates it to text using RNNs to decode signals.
In one of the early experiments, they asked a paralysed subject to &amp;ldquo;attempt&amp;rdquo; to write as if his hand was not paralysed. These are the main insights from the experiment:
 they recorded neural activity and then used PCA to extract the top 3 axes with the highest variance.</description>
    </item>
    
    <item>
      <title>CraftAssist - LLMs on minecraft (?)</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-10-31-craftassist/</link>
      <pubDate>Sat, 19 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-10-31-craftassist/</guid>
      <description>Craftassist is a cool project where the wuthors figured out a way to use natural language to communicate with a minecraft bot via chat. The even manage to give natural instructions to the robot to do certain tasks.
The first version of the bot had 3 main components:
 Parsing incoming text into logical form (&amp;ldquo;action dictionary&amp;rdquo;) This parsed data is then interpreted by a dialogue object (not sure what that is).</description>
    </item>
    
    <item>
      <title>Watching artificial intelligence through the lens of CogSci</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-09-27-ai-through-lens-of-cogsci/</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-09-27-ai-through-lens-of-cogsci/</guid>
      <description>The bias in humans interpreting functional mechanisms of a system A paper mentions a concept called the &amp;ldquo;theory-of-mind&amp;rdquo; (Baron-Cohen et al., 1985) which mentions the fact that humans can understand that other humans have mental states like that of themselves. Our minds make up assumptions about things that are going on inside other people&amp;rsquo;s minds (emotions etc).
The problem is that this bias may also bleed into the way we look into the interpretations we make of a machine.</description>
    </item>
    
    <item>
      <title>Towards a New Science of Common Sense</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-09-03-new-science-of-common-sense/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-09-03-new-science-of-common-sense/</guid>
      <description>Intro Common sense, as we know it has not yet been achieved in machines by any means despite decades of research. Today&amp;rsquo;s large language models still struggle even the smallest of riddles or mental math questions. With LLMs, we have solved language, not logic/common sense.
AI is still very narrow. There are models which are &amp;ldquo;experts&amp;rdquo; at certain tasks, but none posess any general capabilities. AI as we know it today us super good at narrow domains like video games and image captioning.</description>
    </item>
    
    <item>
      <title>Parti - Scaling LLMs for Text to Image tasks</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-08-07-parti-image-generation/</link>
      <pubDate>Sun, 07 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-08-07-parti-image-generation/</guid>
      <description>Scaling Autoregressive Models for Content-Rich Text to Image Generation The architecture itself that&amp;rsquo;s used for parti (that&amp;rsquo;s what the authors call this model) is fairly simple. It&amp;rsquo;s a transformer encoder-decoder architecture paired with a ViT VQGAN in the end to tokenize/detokenize images.
How do we tokenize images? For an autoregressive model to work, we basically have to convert everything to tokens. Tokenizing text is super easy. The problem in this case is that we also have to convert images into a sequence of tokens.</description>
    </item>
    
    <item>
      <title>The Third Wave (?)</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-08-01-the-third-wave/</link>
      <pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-08-01-the-third-wave/</guid>
      <description>I will start by rambling a little about how learning language is harder than you think. Then I&amp;rsquo;ll move on to the main paper itself.
Learning language is harder than you think An interesting point that was made in the article was that a language is learned not entirely through memorization. Sentences like I breaked the window are grammatically invalid, but they still manage to get the meaning accross. We manage to make a meaningful fact tuple i.</description>
    </item>
    
    <item>
      <title>Locating and Editing Factual Knowledge in GPTs</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-19-07-locating-and-editing-factual-associations-in-gpt/</link>
      <pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-19-07-locating-and-editing-factual-associations-in-gpt/</guid>
      <description>This paper&amp;rsquo;s approach has been to develop a mechanism to identify the neuron activations that lead to a model&amp;rsquo;s factual predictions and possibly even edit existing facts.
Key concepts and takeaways: What is a fact tuple? It is a special way to store knowledge in the form of a tuple which looks like the following:
## (subject, relation, object) (&amp;#34;Edmunt Neupert&amp;#34;, &amp;#34;Plays the instrument&amp;#34;, &amp;#34;Piano&amp;#34;) It contains the subject (index 0), the object (index 2) and their relation (index 1).</description>
    </item>
    
    <item>
      <title>Language Models (mostly) Know What they know</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-14-07-lms-mostly-know-what-they-know/</link>
      <pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-14-07-lms-mostly-know-what-they-know/</guid>
      <description>Can we make LMs predict which questions they&amp;rsquo;ll be able to answer correctly? It is important for LLMs to &amp;ldquo;know&amp;rdquo; what they know and what they do not know. The problem is that LMs are generally never trained to say &amp;ldquo;I do not know&amp;rdquo;. But it might be possible to quantify this ability post-training.
This is how they approach the problem:
 Finetune models with a value head to predict the probabiility that they can answer a given question correctly.</description>
    </item>
    
    <item>
      <title>Attention Rollout</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-04-16-attention_flow/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-04-16-attention_flow/</guid>
      <description>How is it better than just viewing raw attention maps ? Viewing raw attention maps as a way to explain transformers does not take into account the fact that we also have residual connections in the model. When we only use attention weights to approximate the flow of information in Transformers, we ignore the residual connections. But these connections play a significant role in tying corresponding positions in different layers.</description>
    </item>
    
    <item>
      <title>Generic Attention-model Explainability</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-04-16-generic_attention_model_explainability/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-04-16-generic_attention_model_explainability/</guid>
      <description>Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers Can be used to explain models like CLIP. This is how it works:
  First, let us define an input image x_image and a list of input texts [a, b, c] where a, b and c can be any strings which can be tokenized and fed into the model.
input_texts = [&amp;#34;a bald man&amp;#34;, &amp;#34;a rocket in space&amp;#34;, &amp;#34;a man&amp;#34;]   We do a forward pass with a tokenized image and text(s) on CLIP, and obtain logits_per_image and logits_per_text.</description>
    </item>
    
    <item>
      <title>Interpreting LMs with contrastive explanations</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-04-16-interpreting-language-models-with-contrastive-explanations/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-04-16-interpreting-language-models-with-contrastive-explanations/</guid>
      <description>Interpreting language models with contrastive explanations interpretability methods commonly used for other NLP tasks like text classification, such as gradient-based saliency maps are not as informative for LM predictions
In general, language modeling has a large output space and a high complexity compared to other NLP tasks; at each time step, the LM chooses one word out of all vocabulary items. This contrasts with text classification (where the output space is smaller).</description>
    </item>
    
    <item>
      <title>Knowledge Neurons</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-04-16-knowledge_neurons/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-04-16-knowledge_neurons/</guid>
      <description>Knowledge Neurons The knowledge neurons paper asks the following question:
Given an output y from an intermediate layer, how can we determine the neurons which contributed the most/least to the prediction ?
The neurons which contribute the most to the &amp;ldquo;fact&amp;rdquo; mentioned by the model are generally tagged as the &amp;ldquo;knowledge neurons&amp;quot;.
Procedure to find knowledge neurons   Produce n different and diverse prompts expressing this fact
  For each prompt, calculate the attribution score of each intermediate neurons</description>
    </item>
    
    <item>
      <title>LOST - Localizing objects with self supervised transformers</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-04-16-lost-unsupervised-object-detection/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-04-16-lost-unsupervised-object-detection/</guid>
      <description>Lost - Localizing objects with self supervised transformers and no labels The core idea is to be able to use the hidden info within transformers to localize objects (&amp;ldquo;subjects&amp;rdquo;) within input images (much like a YOLO model but without further training).
How does it work ?   First, we assume that there&amp;rsquo;s at least one object to be found in the image.
  it relies on a selection of patches that are likely to belong to an object.</description>
    </item>
    
    <item>
      <title>Self-Consistency Improves Chain of Thought Reasoning in LMs</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-04-16-self-consistency-imrpoves-chain-of-thought/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-04-16-self-consistency-imrpoves-chain-of-thought/</guid>
      <description>Self-Consistency Improves Chain of Thought Reasoning in Language Models One issue with large language models has been to find a way to understand and leverage the concept of &amp;ldquo;chain of thought&amp;rdquo; (i.e reasoning).
The idea behind this paper is to make the LLM generate multiple completions (i.e chains of thought) and then select the result with the most highest number of occurrences among the samples.
highest number of occurrences -&amp;gt; &amp;ldquo;self consistency&amp;rdquo;</description>
    </item>
    
    <item>
      <title>Transformer intrepretability beyond attention</title>
      <link>https://mayukhdeb.github.io/notes/post/2022-04-16-transformer_interpretability_beyond_attention/</link>
      <pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/post/2022-04-16-transformer_interpretability_beyond_attention/</guid>
      <description>Disadvantages of attention rollout and LRP Irrelevant tokens often get highlighted
The main challenge in assigning attributions based on attentions is that attentions are combining non-linearly from one layer to the next. The rollout method assumes that attentions are combined linearly and considers paths along the pairwise attention graph. We observe that this method often leads to an emphasis on irrelevant tokens since even average attention scores can be attenuated. The method also fails to distinguish between positive and negative contributions to the decision.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://mayukhdeb.github.io/notes/about/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mayukhdeb.github.io/notes/about/</guid>
      <description>I like to understand how things work and explain it to my friends.</description>
    </item>
    
  </channel>
</rss>