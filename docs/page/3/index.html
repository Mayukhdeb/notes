<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>


  





<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="page-heading">
        
          <h1>Notes</h1>
          
            <p class="author">Mayukh Deb</p>

            
              <div class="abstract">
                <h5>Abstract</h5>
                <p>
                  This is a place to keep my notes, mostly from the papers I read and find interesting.
                </p>
              </div>
            
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
<div class="container" role="main">
    <div class="posts-list">
      
        
      

      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-12-18-clippo-image-and-language-understanding-from-text-only/">
      <h2 class="post-title">CLIPPO: Image and Language understanding from pixels only</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Dec 18, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>CLIP v/s CLIPPO  CLIP trains 2 seperate image and text encoders, each with it&rsquo;s own preprocessing and embeddings. CLIPPO trains a single transformer where the images are sent in as images and the text is also rendered as a image before shoving it into the forward pass  How do they render text as an image?
Text inputs are rendered on blank images, and are subsequently dealt with entirely as images.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-12-18-clippo-image-and-language-understanding-from-text-only/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-11-20-clip-fields/">
      <h2 class="post-title">VPT: Video Pre-Training on Minecraft</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Dec 18, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Pseudo labelling There are 4 main stages in the data collection stage:
  First, they paid some contractors and obtained 2k hours of labelled minecraft gameplay data. This contained the video frames from within the game and the respective player action (key press, mouse movement). This in total gave them 2k hours of data.
  Then they scrape minecraft videos from youtube and keep only the clean ones. Clean = does not contain the face of the streamer in the corner etc.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-11-20-clip-fields/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-11-19-brain-to-text-communication-via-handwriting/">
      <h2 class="post-title">Brain to Text Communication via handwriting</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Nov 19, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>The original paper can be found here.
The authors of this paper have made a BCI that decodes handwriting movements from neural activity and translates it to text using RNNs to decode signals.
In one of the early experiments, they asked a paralysed subject to &ldquo;attempt&rdquo; to write as if his hand was not paralysed. These are the main insights from the experiment:
 they recorded neural activity and then used PCA to extract the top 3 axes with the highest variance.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-11-19-brain-to-text-communication-via-handwriting/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
      </div>
    
    <ul class="pager">
      
        <li class="previous">
          <a href="https://mayukhdeb.github.io/notes/page/2/">&larr; Newer</a>
        </li>
      
      
        <li class="next">
          <a href="https://mayukhdeb.github.io/notes/page/4/">Older &rarr;</a>
        </li>
      
    </ul>
  
</div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2023
      </p>
  </div>
</footer></body>
</html>
