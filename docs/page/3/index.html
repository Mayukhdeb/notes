<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>


  





<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="page-heading">
        
          <h1>Notes</h1>
          
            <p class="author">Mayukh Deb</p>

            
              <div class="abstract">
                <h5>Abstract</h5>
                <p>
                  This is a place to keep my notes, mostly from the papers I read and find interesting.
                </p>
              </div>
            
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
<div class="container" role="main">
    <div class="posts-list">
      
        
      

      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-knowledge_neurons/">
      <h2 class="post-title">Knowledge Neurons</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Knowledge Neurons The knowledge neurons paper asks the following question:
Given an output y from an intermediate layer, how can we determine the neurons which contributed the most/least to the prediction ?
The neurons which contribute the most to the &ldquo;fact&rdquo; mentioned by the model are generally tagged as the &ldquo;knowledge neurons&quot;.
Procedure to find knowledge neurons   Produce n different and diverse prompts expressing this fact
  For each prompt, calculate the attribution score of each intermediate neurons</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-knowledge_neurons/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-lost-unsupervised-object-detection/">
      <h2 class="post-title">LOST - Localizing objects with self supervised transformers</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Lost - Localizing objects with self supervised transformers and no labels The core idea is to be able to use the hidden info within transformers to localize objects (&ldquo;subjects&rdquo;) within input images (much like a YOLO model but without further training).
How does it work ?   First, we assume that there&rsquo;s at least one object to be found in the image.
  it relies on a selection of patches that are likely to belong to an object.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-lost-unsupervised-object-detection/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-self-consistency-imrpoves-chain-of-thought/">
      <h2 class="post-title">Self-Consistency Improves Chain of Thought Reasoning in LMs</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Self-Consistency Improves Chain of Thought Reasoning in Language Models One issue with large language models has been to find a way to understand and leverage the concept of &ldquo;chain of thought&rdquo; (i.e reasoning).
The idea behind this paper is to make the LLM generate multiple completions (i.e chains of thought) and then select the result with the most highest number of occurrences among the samples.
highest number of occurrences -&gt; &ldquo;self consistency&rdquo;</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-self-consistency-imrpoves-chain-of-thought/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
      </div>
    
    <ul class="pager">
      
        <li class="previous">
          <a href="https://mayukhdeb.github.io/notes/page/2/">&larr; Newer</a>
        </li>
      
      
        <li class="next">
          <a href="https://mayukhdeb.github.io/notes/page/4/">Older &rarr;</a>
        </li>
      
    </ul>
  
</div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2022
      </p>
  </div>
</footer></body>
</html>
