<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Attention Rollout - Writing</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/writing/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/writing/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/writing/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Attention Rollout</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
      </div>
      <br>
      
    <h1 id="attention-rollout-for-explaining-transformers">Attention Rollout for explaining transformers</h1>
<h2 id="how-is-it-better-than-just-viewing-raw-attention-maps-">How is it better than just viewing raw attention maps ?</h2>
<p>Viewing raw attention maps as a way to explain transformers does not take into account the fact that we also have residual connections in the model. When we only use attention weights to approximate the flow of information in Transformers, we ignore the residual connections. But these connections play a significant role in tying corresponding positions in different layers.</p>
<h2 id="refined-method">Refined method</h2>
<p>One drawback of the original attention rollout method was that it was focused primarily on tasks which involved predicting only the &ldquo;next&rdquo; token/any process which involved only one forward pass (like predicting pronouns).</p>
<p>In order to make this method work in our use-case i.e generative LM&rsquo;s, we can iteratively keep track of the attention rollout values while generating <code>n</code> tokens. This would enable us to understand how much each generated token is &ldquo;correlated&rdquo; to the original prompt.</p>
<p>On a lower level, the process does the following:</p>
<ol>
<li>
<p>Given <code>m</code> input tokens, we generate <code>n</code> more tokens i.e <code>n</code> forward passes</p>
</li>
<li>
<p>On each step, we register the attention rollout values. For the <code>i</code>th step, the size of the attention rollout matrix would be: <code>[m + i, l]</code> where <code>l</code> = number of layers with attention outputs in the model.</p>
</li>
<li>
<p>Slice every obtained attention matrix along the last dim as <code>[:m, :]</code> (only include values corresponding to the input tokens).</p>
</li>
<li>
<p>Concatenate all of the obtained sliced matrices along dim 0. Thus giving us a matrix <code>M</code> of shape: <code>[n, m, l]</code> where:</p>
<ul>
<li><code>n</code> = number of generated tokens</li>
<li><code>l</code> = number of layers with attention outputs in the model</li>
<li><code>m</code> = number of tokens in original prompt</li>
</ul>
</li>
<li>
<p>Then in order to analyse all the input tokens w.r.t a generated token, we can slice the matrix as <code>M[index_of_generated_token, :, :]</code>.</p>
 <img src = "images/attention_flow/refined_method.png" width = "90%">
</li>
</ol>
<h2 id="examples">Examples</h2>
<h3 id="names">Names</h3>
<table>
<thead>
<tr>
<th>prompt</th>
<th>completion</th>
<th>selected output token</th>
<th>attention rollout values (mean along layer dim)</th>
</tr>
</thead>
<tbody>
<tr>
<td>My name is Putin, I am from</td>
<td>Russia, I am a Russian</td>
<td>Russia</td>
<td><img src = "images/attention_flow/putin_russia.png" width = "80%"></td>
</tr>
<tr>
<td>My name is Abdul, I am from</td>
<td>Pakistan. I am a student of the University of Delhi</td>
<td>Pakistan</td>
<td><img src = "images/attention_flow/abdul_pak.png" width = "80%"></td>
</tr>
</tbody>
</table>
<hr>
<h3 id="question-answering">Question answering</h3>
<pre><code>Context: It is sunny today, which is bad.
Q: How is the weather today ?
A: It is
</code></pre><p>Completion</p>
<pre><code> sunny, but it is not as warm as it was yesterday.
</code></pre><p>Attention rollout visualizations for completion token <code>sunny</code> shows that the top 3 input tokens are:</p>
<ol>
<li><code>sunny</code></li>
<li><code>weather</code></li>
<li><code>How</code></li>
</ol>
<p>The 2D representation of the same visualization also shows us that the attention rollout value of the input token <code>17: weather</code> spikes at layer number 20, while the input token <code>4: sunny</code>'s attention rollout values are relatively more spread out accross the layers.</p>
<table>
<thead>
<tr>
<th>output token</th>
<th>mean</th>
<th>original</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>sunny</code></td>
<td><img src = "images/attention_flow/context_sunny_mean.png" width = "100%"></td>
<td><img src = "images/attention_flow/context_sunny_2d.png" width = "100%"></td>
</tr>
</tbody>
</table>
<hr>
<h3 id="long-prompt">Long prompt</h3>
<p>Prompt:</p>
<pre><code>The grandest and most renowned of all these is the Colosseum at Rome. It was built by David and his son Titus, the conquerors of Egypt.
Q: Who built the Coliseum ?
A:
</code></pre><p>Completion</p>
<pre><code>The Colosseum was built by David
</code></pre><table>
<thead>
<tr>
<th>output token</th>
<th>mean</th>
<th>original</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>David</code></td>
<td><img src = "images/attention_flow/qna_rome.png" width = "100%"></td>
<td><img src = "images/attention_flow/qna_rome_2d.png" width = "100%"></td>
</tr>
</tbody>
</table>
<h2 id="drawbacks">Drawbacks</h2>
<p>A higher attention rollout value does not necessarily infer to the token being &ldquo;important&rdquo; for a prediction. This method in it&rsquo;s raw form tends to be biased towards common english words and punctuations (<code>[&quot;,&quot;, &quot;.&quot;, &quot;\n&quot;, &quot;is&quot;, &quot;:&quot;. &quot;and&quot;]</code>), which might lead to skewed or non-intuitive results. This makes it necessary for the user to filter out these tokens in order to make sense out of the values.</p>
<h2 id="resourceslinks">Resources/links</h2>
<ul>
<li><a href="https://gitlab.aleph-alpha.de/research/attention-rollout/-/tree/master/examples/streamlit">Try it yourself</a></li>
<li><a href="https://arxiv.org/abs/2005.00928">Original paper</a></li>
</ul>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/writing//tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/writing/about">Mayukh Deb</a>
        &copy;
        2022
      </p>
  </div>
</footer></body>
</html>
