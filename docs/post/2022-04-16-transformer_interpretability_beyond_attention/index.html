<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Transformer intrepretability beyond attention  - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notescss/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notescss/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notescss/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Transformer intrepretability beyond attention </h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
      </div>
      <br>
      
    <h1 id="transformer-intrepretability-beyond-attention">Transformer intrepretability beyond attention</h1>
<h2 id="disadvantages-of-attention-rollout-and-lrp">Disadvantages of attention rollout and LRP</h2>
<p>Irrelevant tokens often get highlighted</p>
<p>The main challenge in assigning attributions based on attentions is that attentions are combining non-linearly from one layer to the next. The rollout method assumes that attentions are combined linearly and considers paths along the pairwise attention graph. We observe that this method often leads to an emphasis on irrelevant tokens since even average attention scores can be attenuated. The method also fails to distinguish between positive and negative contributions to the decision.</p>
<p>Transformers apply activation functions other that ReLU, which result in both positive and negative features. Because of the non-positive values, skip connections lead, if not carefully handled, to numerical instabilities. Methods such as LRP for example, tend to fail in such cases.</p>
<h2 id="how-does-this-method-beat-the-methods-mentioned-above-then-">How does this method beat the methods mentioned above then ?</h2>
<ul>
<li>
<p>Filters out negative contributions to the decision (unlike attention rollout)</p>
</li>
<li>
<p>Considers the fact that transformers use non-linearities which have negative output values such as GeLU, where LRPs generally fail.</p>
<ul>
<li>Non-linearities other that ReLU, such as GELU, output both positive and negative values. To address this, LRP propagation can be modified by constructing a subset of indices we consider only the elements that have a positive weighed relevance.</li>
</ul>
</li>
</ul>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/notes/tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notesabout">Mayukh Deb</a>
        &copy;
        2022
      </p>
  </div>
</footer></body>
</html>
