<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>AudioCLIP: Extending CLIP to Image, Text and Audio - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>AudioCLIP: Extending CLIP to Image, Text and Audio</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 20, 2023
  
</span>
      </div>
      <br>
      
    <p>They aim to train a tri-modal model which provides a common embedding space for 3 modalities: Images, text and Audio. The trick lies in adding in the 3rd modality (audio)</p>
<p>The repo can be found <a href="https://github.com/AndreyGuzhov/AudioCLIP">here</a>.</p>
<p>There are 3 components to this model:</p>
<ol>
<li>Image encoder</li>
<li>Text encoder</li>
<li>Audio Encoder (ESResNeXt)</li>
</ol>
<p>The image and the text encoder were taken from the original pre-trained CLIP model. The Audio encoder was then first pre-trained on the AudioSet dataset until it achieved a reasonably high accuracy.</p>
<p>Once they had a solid Audio encoder, they replaced it&rsquo;s classification head layer with another one whose number of output neurons is the same as the size of CLIP&rsquo;s embedding space. This new output head was randomly initialized.</p>
<p>With this new head, they start finetuning the Audio encoder in the CLIP setting, which made it&rsquo;s outputs compatible with the embeddings of CLIP.</p>
<hr>
<p>They took 2 main approaches:</p>
<p><strong>Approach 1: train Audio-encoder only</strong></p>
<p>The parameters of the other 2 encoders (image, text) remain frozen during this phase. The 2 frozen heads serve as teachers to the Audio encoder.</p>
<p><strong>Approach 2: train all 3 encoders</strong>
Reason:</p>
<p>The distribution of images and textual descriptions in the AudioSet dataset does not follow the one from the CLIP dataset. This led to suboptimal performance in downstream tasks</p>
<p>To fix this, they decided to train all of the 3 heads together on the AudioSet Dataset. This provided an additional performance boost.</p>
<p><strong>Hyperparameters</strong>:</p>
<ul>
<li>Optimizer: SGD with nesterov momentum = 0.9 and weight decay = 5e-4</li>
<li>Batch size: 64</li>
</ul>
<p>The small-ness of the batch size seems counterintuitive to me. Because CLIP used a batch size of like 32k. But this proves at least in theory that we can train contrastive models with small batch sizes.</p>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/notes//tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2023
      </p>
  </div>
</footer></body>
</html>
