<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Writing - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Writing</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
<div class="container" role="main">
    <div class="posts-list">
      
        
      

      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-attention_flow/">
      <h2 class="post-title">Attention Rollout</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>How is it better than just viewing raw attention maps ? Viewing raw attention maps as a way to explain transformers does not take into account the fact that we also have residual connections in the model. When we only use attention weights to approximate the flow of information in Transformers, we ignore the residual connections. But these connections play a significant role in tying corresponding positions in different layers.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-attention_flow/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-generic_attention_model_explainability/">
      <h2 class="post-title">Generic Attention-model Explainability</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers Can be used to explain models like CLIP. This is how it works:
  First, let us define an input image x_image and a list of input texts [a, b, c] where a, b and c can be any strings which can be tokenized and fed into the model.
input_texts = [&#34;a bald man&#34;, &#34;a rocket in space&#34;, &#34;a man&#34;]   We do a forward pass with a tokenized image and text(s) on CLIP, and obtain logits_per_image and logits_per_text.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-generic_attention_model_explainability/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-interpreting-language-models-with-contrastive-explanations/">
      <h2 class="post-title">Interpreting LMs with contrastive explanations</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 16, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Interpreting language models with contrastive explanations interpretability methods commonly used for other NLP tasks like text classification, such as gradient-based saliency maps are not as informative for LM predictions
In general, language modeling has a large output space and a high complexity compared to other NLP tasks; at each time step, the LM chooses one word out of all vocabulary items. This contrasts with text classification (where the output space is smaller).</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-04-16-interpreting-language-models-with-contrastive-explanations/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
      </div>
    
    <ul class="pager">
      
      
        <li class="next">
          <a href="https://mayukhdeb.github.io/notes/post/page/2/">Older &rarr;</a>
        </li>
      
    </ul>
  
</div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2022
      </p>
  </div>
</footer></body>
</html>
