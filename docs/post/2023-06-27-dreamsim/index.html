<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>DreamSim - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>DreamSim</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Jun 27, 2023
  
</span>
      </div>
      <br>
      
    <p>Everything related to this paper can be found here: : <a href="https://dreamsim-nights.github.io/">https://dreamsim-nights.github.io/</a></p>
<p><em>A sense of sameness is the very keel and backbone of our thinking</em> - William James, 1890</p>
<p>Our understanding of the visual world depends crucially on our ability to perceive the similarities between different images.</p>
<p>People in vision have tried to solve this problem by introducing metrics like PSNR, SSIM, LPIPS, DISTS etc. The primary weakness of such metrics is that they focus on pixel/patch level information and fail to capture high-level structure.</p>
<p>This problem of not being able to encode high-level information from images has been partially solved by DINO, CLIP. But the authors find out that although these models are good for measuring image-to-image distances, they are not necessarily well aligned to human perception.</p>
<p>In order to train a model whose embeddings are more aligned to human perception, the authors decided to build a dataset.</p>
<h2 id="human-aligned-dataset">Human-aligned dataset</h2>
<p>In order to gather data, they applied something called <strong>Two alternative forced choice (2AFC)</strong>. The idea was simple:</p>
<ol>
<li>Show a participant 3 images: <code>[x, x1, x2]</code> where x1 and x2 are distortions of x.</li>
<li>Ask participant to select the image which is more similar to x.</li>
</ol>
<p>The final dataset that was collected contained 20k such triplets with 7 unanimous votes for each on average.</p>
<h2 id="training-a-model">Training a model</h2>
<p>They finetuned existing image encoders first by adding an mlp head and then with LoRA. The latter outperformed the former. Ensembling some of the top-performing models gave them the best possible human-alignment score (2AFC score).</p>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/notes//tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about"></a>
        &copy;
        2023
      </p>
  </div>
</footer></body>
</html>
