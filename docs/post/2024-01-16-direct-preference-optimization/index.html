<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Direct Preference Optimization for Diffusion Models - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.92.2" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Direct Preference Optimization for Diffusion Models</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Jan 16, 2024
  
</span>
      </div>
      <br>
      
    <p>DPO aims to overcome the primary drawbacks of RLHF, which are it&rsquo;s unstable nature and the dependence on a reward model trained on human preference data.</p>
<p>In <a href="https://arxiv.org/abs/2311.12908">this paper</a>, they perform DPO on diffusion models by training the model to alter it&rsquo;s denoising directions towards preferred images over non-preferred ones. For this objective, they use the <a href="https://huggingface.co/datasets/yuvalkirstain/pickapic_v2/viewer/default/train">pick-a-pic v2</a> dataset.</p>
<p>I&rsquo;ll be using <a href="https://github.com/huggingface/diffusers/blob/main/examples/research_projects/diffusion_dpo/train_diffusion_dpo_sdxl.py">this script</a> as a reference. Note that in the aforementioned paper, they finetune the entire model but in the script we see that they&rsquo;re fine-tuning a LoRA adapter on the model.</p>
<h1 id="dataset">Dataset</h1>
<p>The  <a href="https://huggingface.co/datasets/yuvalkirstain/pickapic_v2/viewer/default/train">pick-a-pic v2</a> was collected by showing users 2 images and then asking them to pick the better one (with an optional neutral response). The image that&rsquo;s not better is eliminated and is replaced by another image generated by the same prompt. The user also got an option to switch to a different prompt and start afresh.</p>
<p>It has the following relevant columns:</p>
<ol>
<li><code>caption</code>: prompt that was used to generate the image</li>
<li><code>jpg_0</code>: first image shown to the user</li>
<li><code>jpg_1</code>: second image shown to the user</li>
<li><code>label_0</code>: is <code>1</code> if the user picked this image to be the better one</li>
<li><code>label_1</code>: is <code>1</code> if the user picked this image to be the better one</li>
</ol>
<p>One can imagine how a single batch of images would look like (without shuffling the dataset):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#f92672">&#34;images&#34;</span>: [
        <span style="color:#960050;background-color:#1e0010">&lt;caption_</span><span style="color:#ae81ff">0</span><span style="color:#960050;background-color:#1e0010">_jpg_</span><span style="color:#ae81ff">0</span><span style="color:#960050;background-color:#1e0010">&gt;</span>,   <span style="color:#75715e">// winning image
</span><span style="color:#75715e"></span>        <span style="color:#960050;background-color:#1e0010">&lt;caption_</span><span style="color:#ae81ff">1</span><span style="color:#960050;background-color:#1e0010">_jpg_</span><span style="color:#ae81ff">0</span><span style="color:#960050;background-color:#1e0010">&gt;</span>,
        <span style="color:#960050;background-color:#1e0010">&lt;caption_</span><span style="color:#ae81ff">0</span><span style="color:#960050;background-color:#1e0010">_jpg_</span><span style="color:#ae81ff">1</span><span style="color:#960050;background-color:#1e0010">&gt;</span>,
        <span style="color:#960050;background-color:#1e0010">&lt;caption_</span><span style="color:#ae81ff">1</span><span style="color:#960050;background-color:#1e0010">_jpg_</span><span style="color:#ae81ff">1</span><span style="color:#960050;background-color:#1e0010">&gt;</span>   <span style="color:#75715e">// winning image
</span><span style="color:#75715e"></span>    ],
    <span style="color:#f92672">&#34;labels&#34;</span>: [<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>]
}
</code></pre></div><h1 id="basics">Basics</h1>
<p>This is a tiny guide to how atent diffusion models are trained.</p>
<p><img src="https://github.com/Mayukhdeb/notes/blob/master/content/images/2024-01-16-direct-preference-optimization/difusion_training_objective.png?raw=true" alt="Diffusion Breakdown"></p>

Add noise \(\eta\) to the latent of the original image. The amount of noise is proportional to the timestep.


Feed the noisy latent into a model. The model tries to predict the noise, which gives us \(\eta_{pred}\)

<p>Finally, the generated sample is the noisy sample minus the predicted noise.</p>
<h1 id="single-training-step">Single Training Step</h1>
<ol>
<li>
<p>First, we extract the all the latent vectors for each image in the batch using a pre-trained VAE. Let&rsquo;s call them <code>latents</code></p>
</li>
<li>
<p>Then we add some noise to the latents.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(latents)<span style="color:#f92672">.</span>chunk(<span style="color:#ae81ff">2</span>)[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>repeat(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</code></pre></div><p>Something interesting is going on here. Let&rsquo;s take a deeper look into each part.</p>
<ul>
<li><code>.chunk(2)[0]</code>: divides the tensor into 2 chunks (along dim 0) and then select the first one (see appendix: 1).</li>
<li><code>.repeat(2, 1, 1, 1)</code>: repeats the tensor 2 times along dim <code>0</code> and no repeats along every other dim.</li>
</ul>
<p>This would make sure that images generated by the same caption have the same noise added to the latents.</p>
</li>
<li>
<p>The model takes the noisy latents and the prompt embeddings as input and predicts the noise present in the latents.</p>
</li>
<li>
<p>MSE loss (<code>model_losses</code>) is computed between the predicted noise and the actual noise without any reduction. We get a tensor of shape <code>batch_size</code> containing the MSE loss corresponding to each batch item.</p>
</li>
<li>
<p><code>model_losses</code> is divided into two chunks, one containing the losses for all winning samples (<code>model_losses_w</code>) and another containing the losses for all losing samples (<code>model_losses_l</code>).</p>
</li>
<li>
<p>Then we calculate a term <code>model_diff = model_losses_w - model_losses_l</code>. Note that it <strong>if <code>model_diff</code> is minimized, we guide the model&rsquo;s denoising process towards generating winning samples and away from generating losing samples</strong>.</p>
</li>
</ol>
<p>The diagram shown below is a visualization of <code>model_losses_w</code> and <code>model_losses_l</code> 
 as \(loss_w\) and \(loss_l\) respectively and \(\eta\) as noise added to the image latents.</p>
<p><img src="https://github.com/Mayukhdeb/notes/blob/master/content/images/2024-01-16-direct-preference-optimization/winning_and_losing_sample_losses.png?raw=true" alt="Diffusion Breakdown"></p>
<ol start="7">
<li>
<p>We temporarily disable the LoRA adapters in the model and obtain the predicted noise from the original pre-trained model and calculate <code>ref_diff</code> which is equivalent to <code>model_diff</code> but for the original model.</p>
</li>
<li>
<p>The final loss that is to be minimized is calculated as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">inside_term <span style="color:#f92672">=</span> scale_term <span style="color:#f92672">*</span> (ref_diff <span style="color:#f92672">-</span> model_diff)
loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> F<span style="color:#f92672">.</span>logsigmoid(inside_term<span style="color:#f92672">.</span>mean())
</code></pre></div><p>$$
let, f(x) = -log(sigmoid(x))
$$</p>
<p>$$
\lim_{{x \to (-\infty)}} f(x) = +\infty
$$</p>
<p>$$
\lim_{{x \to (+\infty)}} f(x) = 0
$$</p>
<p>Hence in order to mimimize the <code>loss</code>, we have to maximize the <code>inside_term</code> (for a visualization, see appendix: 2). This can be done by:</p>
<ol>
<li>Maximizing <code>ref_diff</code> (but that is not possible since the original model is frozen)</li>
<li>Minimizing  <code>model_diff</code> i.e steer the model&rsquo;s denoising process towards the winning samples and away from losing samples</li>
</ol>
</li>
</ol>
<p><img src="https://github.com/Mayukhdeb/notes/blob/master/content/images/2024-01-16-direct-preference-optimization/summary.png?raw=true" alt="Diffusion Breakdown"></p>
<h2 id="appendix">Appendix</h2>
<ol>
<li>
<p>Let&rsquo;s imagine <code>x</code> to be a 1d tensor: <code>[4, 5, 6, 7]</code>, then the operation <code>x.chunk(2)[0].repeat(2)</code> would give us the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">&gt;&gt;&gt; x <span style="color:#f92672">=</span> torch.tensor<span style="color:#f92672">([</span>4, 5, 6, 7<span style="color:#f92672">])</span>
&gt;&gt;&gt; x.chunk<span style="color:#f92672">(</span>2<span style="color:#f92672">)</span>
<span style="color:#f92672">(</span>tensor<span style="color:#f92672">([</span>4, 5<span style="color:#f92672">])</span>, tensor<span style="color:#f92672">([</span>6, 7<span style="color:#f92672">]))</span>
&gt;&gt;&gt; x.chunk<span style="color:#f92672">(</span>2<span style="color:#f92672">)[</span>0<span style="color:#f92672">]</span>
tensor<span style="color:#f92672">([</span>4, 5<span style="color:#f92672">])</span>
&gt;&gt;&gt; x.chunk<span style="color:#f92672">(</span>2<span style="color:#f92672">)[</span>0<span style="color:#f92672">]</span>.repeat<span style="color:#f92672">(</span>2<span style="color:#f92672">)</span>
tensor<span style="color:#f92672">([</span>4, 5, 4, 5<span style="color:#f92672">])</span>
</code></pre></div></li>
<li>
<p>This is how  -log(sigmoid(x))  looks like:</p>
 <img src = "https://github.com/Mayukhdeb/notes/blob/master/content/images/2024-01-16-direct-preference-optimization/minus_log_sigmoid.png?raw=true" width = "50%"></li>
</ol>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/notes//tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about"></a>
        &copy;
        2024
      </p>
  </div>
</footer></body>
</html>
