<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Writing - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Writing</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
<div class="container" role="main">
    <div class="posts-list">
      
        
      

      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-12-24-clip-fields/">
      <h2 class="post-title">CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Dec 24, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>What are they doing? They found a way to help a robot make a &ldquo;map&rdquo; of the world around it in terms of multimodal scene encodings. Then they store these multimodal scene encodings and their respective labels (&ldquo;chair&rdquo;) on a database which is differentiable and is also easily searchable.
How are they doing it?  In order to collect data, they used an RGB-D from which the following data was collected:   RGB-D data bounding boxes from pre-trained object detector  the labels of the bounding boxes are fed into a BERT the bounding boxes on the image are used to crop it and feed each crop into CLIP and store the CLIP encoding   Spatial location (x, y, z)  They used an iphone 13 pro with a LiDAR sensor for depth image sequences.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-12-24-clip-fields/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-12-18-clippo-image-and-language-understanding-from-text-only/">
      <h2 class="post-title">CLIPPO: Image and Language understanding from pixels only</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Dec 18, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>CLIP v/s CLIPPO  CLIP trains 2 seperate image and text encoders, each with it&rsquo;s own preprocessing and embeddings. CLIPPO trains a single transformer where the images are sent in as images and the text is also rendered as a image before shoving it into the forward pass  How do they render text as an image?
Text inputs are rendered on blank images, and are subsequently dealt with entirely as images.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-12-18-clippo-image-and-language-understanding-from-text-only/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-11-20-clip-fields/">
      <h2 class="post-title">VPT: Video Pre-Training on Minecraft</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Dec 18, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Pseudo labelling There are 4 main stages in the data collection stage:
  First, they paid some contractors and obtained 2k hours of labelled minecraft gameplay data. This contained the video frames from within the game and the respective player action (key press, mouse movement). This in total gave them 2k hours of data.
  Then they scrape minecraft videos from youtube and keep only the clean ones. Clean = does not contain the face of the streamer in the corner etc.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-11-20-clip-fields/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
      </div>
    
    <ul class="pager">
      
        <li class="previous">
          <a href="https://mayukhdeb.github.io/notes/post/page/1/">&larr; Newer</a>
        </li>
      
      
        <li class="next">
          <a href="https://mayukhdeb.github.io/notes/post/page/3/">Older &rarr;</a>
        </li>
      
    </ul>
  
</div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2023
      </p>
  </div>
</footer></body>
</html>
