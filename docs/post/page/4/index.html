<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Writing - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Writing</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
<div class="container" role="main">
    <div class="posts-list">
      
        
      

      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-12-25-diffusion/">
      <h2 class="post-title">Order From Chaos (Part 1): Diffusion for image synthesis explained in simple words</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Dec 25, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Warning: This post is still being written and is not complete, I just uploaded a draft.
Intro If you take and image and iteratively add very small amounts of noise to it, eventually the image would be unrecongnizable to the eye &ndash; Now what if we could undo this process?
Start from noise, and then iteratively remove the noise until you end up with the real image again.
The Forward and backward processes Given an image \(X_0\), we would want to convert it to \(X_T\) by gradually adding noise to \(X_0\) in \(T\) time steps.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-12-25-diffusion/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-12-25-diffusion-code/">
      <h2 class="post-title">Order From Chaos (Part 2): Diffusion for image synthesis explained in code and a little bit of math</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Dec 25, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>Warning: This post is still being written and is not complete, I just uploaded a draft.
This post is basically what I learned while watching this video by DeepFindr.
Diffusion models work by destroying an input gradually until it looks like noise and then recovering the input image from that. The forward process is hardcoded, and the reverse process is trainable.
In the reverse process, the task of the model is to predict the noise that was added in each step to the input image.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-12-25-diffusion-code/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2022-12-24-clip-fields/">
      <h2 class="post-title">CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Dec 24, 2022
  
</span>
  </div>
  <div class="post-entry">
    
      <p>What are they doing? They found a way to help a robot make a &ldquo;map&rdquo; of the world around it in terms of multimodal scene encodings. Then they store these multimodal scene encodings and their respective labels (&ldquo;chair&rdquo;) on a database which is differentiable and is also easily searchable.
How are they doing it?  In order to collect data, they used an RGB-D from which the following data was collected:   RGB-D data bounding boxes from pre-trained object detector  the labels of the bounding boxes are fed into a BERT the bounding boxes on the image are used to crop it and feed each crop into CLIP and store the CLIP encoding   Spatial location (x, y, z)  They used an iphone 13 pro with a LiDAR sensor for depth image sequences.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2022-12-24-clip-fields/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
      </div>
    
    <ul class="pager">
      
        <li class="previous">
          <a href="https://mayukhdeb.github.io/notes/post/page/3/">&larr; Newer</a>
        </li>
      
      
        <li class="next">
          <a href="https://mayukhdeb.github.io/notes/post/page/5/">Older &rarr;</a>
        </li>
      
    </ul>
  
</div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about"></a>
        &copy;
        2023
      </p>
  </div>
</footer></body>
</html>
