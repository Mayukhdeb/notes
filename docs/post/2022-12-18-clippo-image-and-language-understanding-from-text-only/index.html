<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>CLIPPO: Image and Language understanding from pixels only - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>CLIPPO: Image and Language understanding from pixels only</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Dec 18, 2022
  
</span>
      </div>
      <br>
      
    <h2 id="clip-vs-clippo">CLIP v/s CLIPPO</h2>
<ul>
<li>CLIP trains 2 seperate image and text encoders, each with it&rsquo;s own preprocessing and embeddings.</li>
<li>CLIPPO trains a single transformer where the images are sent in as images and the text is also rendered as a image before shoving it into the forward pass</li>
</ul>
<p><strong>How do they render text as an image?</strong></p>
<p>Text inputs are rendered on blank images, and are subsequently dealt with entirely as images. They literally just &ldquo;write&rdquo; the text on a blank image and then just treat it as an image. An accidental advantage of this kind of a method is that it removes the need for a tokenizer.</p>
<p>A common loss objecive used in these kinds of tasks is the contrastive loss.</p>
<h2 id="what-is-a-contrastive-loss">What is a contrastive loss?</h2>
<p>When training models on Image/alt-text pairs, two encoders are usually trained with a contrastive loss, encouraging the embeddings of corresponding images and alt-text to be similar, and at the same time to be dissimilar from all other image and alt-text embeddings.</p>
<h2 id="my-noob-opinions">My noob opinions</h2>
<ul>
<li>in a way, CLIPPO is the anti-MAGMA. They adopt the text to the image space, while MAGMA adopts the image to the text (embedding) space (but I&rsquo;m not sure how much of a modality gap[1] we have on MAGMA)</li>
</ul>
<p><strong>Questions</strong></p>
<ul>
<li>what about text prompts which are too long? what do you do then?</li>
<li>can we move smoothly between visual concepts by interpolating the &ldquo;image containing the text&rdquo;?</li>
</ul>
<h2 id="references">References:</h2>
<ol>
<li><a href="https://arxiv.org/pdf/2203.02053.pdf">Interesting related paper on &ldquo;modality gaps&rdquo;</a></li>
</ol>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/notes//tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2023
      </p>
  </div>
</footer></body>
</html>
