<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>Brain to Text Communication via handwriting - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="post-heading">
        
          <h1>Brain to Text Communication via handwriting</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
  <div class="container" role="main">
    <article class="article" class="blog-post">
      <div class="postmeta">
        <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Nov 19, 2022
  
</span>
      </div>
      <br>
      
    <p>The original paper can be found <a href="https://www.nature.com/articles/s41586-021-03506-2">here</a>.</p>
<p>The authors of this paper have made a BCI that decodes handwriting movements from neural activity and translates it to text using RNNs to decode signals.</p>
<p>In one of the early experiments, they asked a paralysed subject to &ldquo;attempt&rdquo; to write as if his hand was not paralysed. These are the main insights from the experiment:</p>
<ul>
<li>they recorded neural activity and then used PCA to extract the top 3 axes with the highest variance.</li>
<li>when they used t-SNE, it was also revealed that &ldquo;characters which are written similarly are closer together&rdquo;</li>
<li>overall takeaway: even after years of paralysis, the neural representations of handwriting in the brain is strong enough to be useful in a BCI</li>
</ul>
<h2 id="further-investigation-on-another-related-paperhttpswwwfrontiersinorgarticles103389fnhum2021653659full">Further investigation on <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2021.653659/full">another related paper</a></h2>
<p>In this paper, they use LM-like models to deocde EEG data. They see that a single pre-trained generalised pretty well accross different subjects.</p>
<p>In the field of BCI, it has been generally seen that shallow NNs are more effective than their deeper counterparts. So we just train a new small NN for each user. Even though shallow NNs are good when trained on one person, it is seen that deeper NNs suck less when theyre trained on person A and tested on person B when compared to shallow NNs.</p>
<p>We might be able to exploit the tranferable nature of the first few layers of DNNs in BCI too.</p>
<p>The authors have developed a framework where arbitrary EEG segments are encoded as a seq of learned vectors. They basically made EEG -&gt; Embeddings. Problems like sleep-stage classification involves smooth changes in the signals within the brain. White EEG data from other tasks might be different.</p>
<p>Notes on training the transformer:</p>
<ol>
<li>They first train a transformer-encoder</li>
<li>They use a weight init scheme known as T-Fixup</li>
<li>8 layers, with 8 heads, model dimension of 1536 and an internal feed-forward dimension of 3076</li>
<li>Represent position using an additive (grouped) conv layer</li>
<li>Cosine learning rate decay</li>
<li>Linear warm-up for 5 and 10% of total training steps (batches) for pre-training and fine-tuning</li>
<li>The pre-training procedure largely follows that of <a href="https://pytorch.org/tutorials/intermediate/speech_recognition_pipeline_tutorial.html">wav2vec 2.0</a></li>
</ol>
<p>WIP</p>



      
        <div class="blog-tags">
          
            <a href="https://mayukhdeb.github.io/notes//tags/paper/">paper</a>&nbsp;
          
        </div>
      
    </article>
    
  </div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about">Mayukh Deb</a>
        &copy;
        2022
      </p>
  </div>
</footer></body>
</html>
