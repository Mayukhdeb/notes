<!DOCTYPE html>
<html lang="en"><meta charset="utf-8" />

  <title>paper - Notes</title>


<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/latex.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/main.css" />
<link rel="stylesheet" href="https://mayukhdeb.github.io/notes/css/darkmode.css" />
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="generator" content="Hugo 0.68.3" /><body>






<header>
  <nav class="navbar">
  <div class="nav">
    

    <ul class="nav-links">
      
    </ul>
  </div>
</nav>
  <div class="intro-header">
    <div class="container">
      <div class="tags-heading">
        
          <h1>paper</h1>
          
        
      </div>
    </div>
  </div>
</header>
<div id="content">
<div class="container" role="main">
    <div class="posts-list">
      
        
      

      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2023-04-20-audioclip/">
      <h2 class="post-title">AudioCLIP: Extending CLIP to Image, Text and Audio</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 20, 2023
  
</span>
  </div>
  <div class="post-entry">
    
      <p>They aim to train a tri-modal model which provides a common embedding space for 3 modalities: Images, text and Audio. The trick lies in adding in the 3rd modality (audio)
The repo can be found here.
There are 3 components to this model:
 Image encoder Text encoder Audio Encoder (ESResNeXt)  The image and the text encoder were taken from the original pre-trained CLIP model. The Audio encoder was then first pre-trained on the AudioSet dataset until it achieved a reasonably high accuracy.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2023-04-20-audioclip/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2023-04-04-things-meg-dataset/">
      <h2 class="post-title">THINGS: a multimodal dataset for investigating object representations in thehuman brain</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Apr 4, 2023
  
</span>
  </div>
  <div class="post-entry">
    
      <p>It&rsquo;s actually a collection of 3 datasets. All of which can be found here. The images used can be found here.
The 3 datasets are:
 FMRI scans from 3 participants. 8740 images. MEG scans. 4 participants. 22k images. 12k participants. 4.7 million similarity judgements.  The 2 main datasets I&rsquo;m interested in are the MEG scans and the similarity judgements. For the MEG scans, we can use these scripts as boilerplate.</p>
        <a href="https://mayukhdeb.github.io/notes/post/2023-04-04-things-meg-dataset/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
        <article class="post-preview">
  <a href="https://mayukhdeb.github.io/notes/post/2023-01-25-open-vocab-eeg-text-decoding/">
      <h2 class="post-title">Open Vocabulary EEG-To-Text Decoding</h2>
  </a>
  <div class="postmeta">
    <span class="meta-post">
  <i class="fa fa-calendar-alt"></i>Mar 9, 2023
  
</span>
  </div>
  <div class="post-entry">
    
      <p>The primary problem with most of the existing Brain-Signal to text pipelines is that they have a closed vocabulary.
In this paper, they use pretrained language models for EEG-To-Text decoding and make a zeroshot pilpieline for sentiment classification from EEG. The interesting part is that it can can leverage data from various subjects and sources, i.e huge potential for EEG to text systems with enough data.
Interesting quote from paper:</p>
        <a href="https://mayukhdeb.github.io/notes/post/2023-01-25-open-vocab-eeg-text-decoding/" class="post-read-more">Read More</a>
    
  </div>
</article>
      
      </div>
    
    <ul class="pager">
      
        <li class="previous">
          <a href="https://mayukhdeb.github.io/notes/tags/paper/page/1/">&larr; Newer</a>
        </li>
      
      
        <li class="next">
          <a href="https://mayukhdeb.github.io/notes/tags/paper/page/3/">Older &rarr;</a>
        </li>
      
    </ul>
  
</div>

        </div><footer>
  <div class="container">
    <p class="credits copyright">
      <p class="credits theme-by">
        Powered By <a href="https://gohugo.io">Hugo</a>&nbsp;/&nbsp;Theme&nbsp;<a href="https://github.com/HelloRusk/HugoTeX">HugoTeX</a>
        <br>
        <a href="https://mayukhdeb.github.io/notes/about"></a>
        &copy;
        2023
      </p>
  </div>
</footer></body>
</html>
