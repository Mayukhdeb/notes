<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>MindEye: fMRI-to-Image with Contrastive Learning </title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">MindEye: fMRI-to-Image with Contrastive Learning </h1>
</header>
<h2 id="what-are-they-doing">What are they doing?</h2>
<p>The authors found a way to retrieve and reconstruct images from brain signals (FMRI). The 2 aspectes i.e retrieval and reconstruction are handled by 2 different modules.</p>
<ol type="1">
<li>For retrieval, they use contrastive learning.</li>
<li>For reconstruction, they use diffusion.</li>
</ol>
<p>Note that retrieval means mapping FMRI signals to a useful embedding space like that of CLIP image emebddings. Which can be used for similarity-search type tasks.</p>
<p>Key insights:</p>
<ol type="1">
<li>Using really big linear layers do not lead to overfitting, instead it directly improve the model’s performance. (I believe this is for the retrieval part)</li>
<li>A bidirectional version of mixup contrastive data augmentation improves model performance in a low-sample setting (like that of ours)</li>
<li>CLIP embeddings are <em>not</em> good at holding perceptual information (like locations of objects, colors etc). This is because CLIP image encoders were trained to maximise similarity with text.</li>
<li>To solve the problem that arises in (3), the authors also build a low-level pipeline which preserves low level perceptual information, which would be useful for reconstruction.</li>
</ol>
<p>Other important things - All of their models are trained on a single A100 GPU with a batch size of 32 - They used the Natural Scenes dataset.</p>
<p>Contrastive training</p>
<ol type="1">
<li>They use CLIP-loss as their training objective.</li>
<li>They use fancy techniques like:
<ul>
<li><a href="https://arxiv.org/abs/1710.09412">Mixup</a>: trains models on synthetic data created through convex combinations of two datapoint-label pairs</li>
<li><a href="https://arxiv.org/pdf/2010.06300.pdf">MixCo</a>: mixup + InfoNCE Loss (this shows improved classification performance)</li>
<li>They also modify the CLIP-loss to utilize MixCo</li>
</ul></li>
</ol>
<p>Things that I do not understand well:</p>
<ul>
<li>Mixup</li>
<li>MixCo</li>
<li><a href="https://proceedings.neurips.cc/paper_files/paper/2000/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Paper.pdf">Vicinal Risk minimization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Structural_risk_minimization">Structural Risk Minimization</a></li>
</ul>
<h2 id="what-is-mixup">What is Mixup?</h2>
<p>Traditional deep-learning trains a model F(x) to return a y such that it is as close to the label as possible.</p>
<p>So if there’s a dataset with 2 samples, say [(x1, l1) and (x2,l2)] then we train our models on only these 2 datapoints and nothing else.</p>
<p>But in Mixup, we train not only on the 2 dataset samples that we have, but also on N datapoints which lie “in between” x1 and x2. We also interpolate the labels accordingly as we do this.</p>
<h2 id="what-is-mixco">What is MixCo?</h2>
<p>In contrastive learning, we train a model to embed the representations of differently augmented versions of the same image (positive pair) to be similar, while making them dissimilar if they came from different images (negative pair).</p>
<p>MixCo extends the idea of mixup to the domain of contrastive training objectives.</p>
<p>We “mix” images by generating <em>semi-positive</em> pairs since the images before mixing are originally negative pairs of each other.</p>
<h2 id="useful-extra-links">Useful extra links:</h2>
<ol type="1">
<li>Great video on Mixup: https://www.youtube.com/watch?v=hGAKHKqmXdY</li>
<li>Yannic’s video on Mixup: https://www.youtube.com/watch?v=a-VQfQqIMrE</li>
<li>Blog post on InfoNCE: https://jxmo.io/posts/nce</li>
</ol>
</body>
</html>
