<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>CLIPPO: Image and Language understanding from pixels only</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">CLIPPO: Image and Language understanding from pixels only</h1>
</header>
<h2 id="clip-vs-clippo">CLIP v/s CLIPPO</h2>
<ul>
<li>CLIP trains 2 seperate image and text encoders, each with it’s own preprocessing and embeddings.</li>
<li>CLIPPO trains a single transformer where the images are sent in as images and the text is also rendered as a image before shoving it into the forward pass</li>
</ul>
<p><strong>How do they render text as an image?</strong></p>
<p>Text inputs are rendered on blank images, and are subsequently dealt with entirely as images. They literally just “write” the text on a blank image and then just treat it as an image. An accidental advantage of this kind of a method is that it removes the need for a tokenizer.</p>
<p>A common loss objecive used in these kinds of tasks is the contrastive loss.</p>
<h2 id="what-is-a-contrastive-loss">What is a contrastive loss?</h2>
<p>When training models on Image/alt-text pairs, two encoders are usually trained with a contrastive loss, encouraging the embeddings of corresponding images and alt-text to be similar, and at the same time to be dissimilar from all other image and alt-text embeddings.</p>
<h2 id="my-noob-opinions">My noob opinions</h2>
<ul>
<li>in a way, CLIPPO is the anti-MAGMA. They adopt the text to the image space, while MAGMA adopts the image to the text (embedding) space (but I’m not sure how much of a modality gap[1] we have on MAGMA)</li>
</ul>
<p><strong>Questions</strong> - what about text prompts which are too long? what do you do then? - can we move smoothly between visual concepts by interpolating the “image containing the text”?</p>
<h2 id="references">References:</h2>
<ol type="1">
<li><a href="https://arxiv.org/pdf/2203.02053.pdf">Interesting related paper on “modality gaps”</a></li>
</ol>
</body>
</html>
