<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Generic Attention-model Explainability</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Generic Attention-model Explainability</h1>
</header>
<p>+++ title = “Generic Attention-model Explainability” author = “Mayukh Deb” tags = [“paper”] date = “2022-04-16” +++</p>
<h2 id="generic-attention-model-explainability-for-interpreting-bi-modal-and-encoder-decoder-transformers">Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers</h2>
<p>Can be used to explain models like CLIP. This is how it works:</p>
<ol type="1">
<li><p>First, let us define an input image <code>x_image</code> and a list of input texts <code>[a, b, c]</code> where <code>a</code>, <code>b</code> and <code>c</code> can be any strings which can be tokenized and fed into the model.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a>input_texts <span class="op">=</span> [<span class="st">&quot;a bald man&quot;</span>, <span class="st">&quot;a rocket in space&quot;</span>, <span class="st">&quot;a man&quot;</span>]</span></code></pre></div></li>
<li><p>We do a forward pass with a tokenized image and text(s) on CLIP, and obtain <code>logits_per_image</code> and <code>logits_per_text</code>. Each item within these tensors correspond to <code>a</code> and <code>b</code> and <code>c</code> respectvely.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>{</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>    <span class="st">&#39;logits_per_image&#39;</span>: tensor([[<span class="fl">22.7188</span>, <span class="fl">16.1094</span>, <span class="fl">23.4219</span>]]), <span class="co">## shape: 1,3</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>    <span class="st">&#39;logits_per_text&#39;</span>: tensor([[<span class="fl">22.7344</span>],[<span class="fl">16.1250</span>],[<span class="fl">23.4219</span>]]) <span class="co">## shape: 3,1</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>}</span></code></pre></div></li>
<li><p>We run a softmax over <code>logits_per_image</code> on the last dim and get a nice probability map with a sum of 1 (we also convert it to numpy). Let’s call it <code>probs</code>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>{</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>    <span class="st">&#39;probs&#39;</span>: array([[<span class="fl">3.311e-01</span>, <span class="fl">4.461e-04</span>, <span class="fl">6.685e-01</span>]]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>}</span></code></pre></div></li>
<li><p>We then choose a target <code>index</code> w.r.t which we want to find the relevance mapping. The <code>index</code> here refers to the index of one the input texts which refers to <code>a</code>, <code>b</code> or <code>c</code>. For now, let’s set <code>index = 0</code></p></li>
<li><p>Now, we create a tensor with all zeros called <code>one_hot</code> which has the same shape as <code>logits_per_image</code> i.e <code>(1, 3)</code> and set <code>one_hot[0, index] = 0</code>. This tensor has <code>requires_grad</code> set to true which will come to play later on:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>{</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>    <span class="st">&#39;one_hot&#39;</span>: tensor([[<span class="fl">1.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>]], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a>}</span></code></pre></div></li>
<li><p>We calculate a “loss” which is defined as shown below and do a backward pass over through the model. <code>one_hot * logits_per_image</code> masks <code>logits_per_image</code> to only include the logit corresponding to <code>index</code>. So in this case <code>loss = 22.7188</code></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a>loss <span class="op">=</span> torch.<span class="bu">sum</span>(one_hot <span class="op">*</span> logits_per_image)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a>model.zero_grad()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>loss.backward(retain_graph<span class="op">=</span><span class="va">True</span>)</span></code></pre></div></li>
<li><p>Now, let us assume that we have <code>n</code> layers in the model with attention outputs. Out of which we select the last layer for our work, let us call it <code>block</code>. The output of the attention layer would be a 3D tensor of shape which looks something like: <code>[12, 50, 50]</code>. We select a subset of these layers for our experiments.</p></li>
<li><p>We make an identity matrix <code>R</code> (probably means to “results”) of shape <code>(d , d)</code> where <code>d</code> is the length of the last dim of the attention outputs of the model. In this case, <code>d</code> is 50.</p></li>
<li><p>Using forward and backward hooks, we capture the outputs of the attention layer(s) into 2 attributes both of which have shape <code>(12, 50, 50)</code>:</p>
<ul>
<li><code>block.attn_probs</code> (forward hook): stores the attention outputs post softmax + dropout.</li>
<li><code>block.attn_grad</code> (backward hook): stores the gradients of the same outputs w.r.t the loss. It tells us <code>d(loss)/d(block.attn_probs)</code>.</li>
</ul></li>
<li><p>we do the following with <code>block.attn_probs</code> and <code>block.attn_grad</code> iteratively for each <code>block</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a>y <span class="op">=</span> block.attn_probs <span class="op">*</span> block.attn_grad  <span class="co">## y.shape = (12, 50, 50)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a>y <span class="op">=</span> cam.clamp(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>).mean(dim<span class="op">=</span><span class="dv">0</span>)  <span class="co">## y.shape = (50,50)</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a>R <span class="op">+=</span> torch.matmul(cam, R)  <span class="co">## R.shape = (50,50) </span></span></code></pre></div></li>
<li><p>We then set <code>R[0, 0] = 0</code> for some reason which I’m not sure of rn.</p></li>
<li><p>Then we obtain a 2D heatmap by selecting a specific subset of <code>R</code> which is <code>R[0, 1:]</code> (which comes to be of shape <code>(49,)</code>) and reshape it to <code>(7,7)</code>.</p></li>
<li><p>Finally, we scale this heatmap and resize it to be of the same height and width as of the original input image.</p></li>
</ol>
<h2 id="resources">Resources:</h2>
<ul>
<li><a href="https://arxiv.org/pdf/2103.15679.pdf">original paper</a></li>
<li><a href="https://github.com/hila-chefer/Transformer-MM-Explainability">original implementation on github</a></li>
</ul>
</body>
</html>
