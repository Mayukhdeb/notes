<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>The Third Wave (?)</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">The Third Wave (?)</h1>
</header>
<p>+++ title = “The Third Wave (?)” date = “2022-08-01” author = “Mayukh Deb” tags = [“paper”] +++</p>
<p>I will start by rambling a little about how <a href="https://garymarcus.substack.com/p/learning-language-is-harder-than">learning language is harder than you think</a>. Then I’ll move on to the main paper itself.</p>
<h2 id="learning-language-is-harder-than-you-think">Learning language is harder than you think</h2>
<p>An interesting point that was made in the article was that a language is learned <em>not</em> entirely through memorization. Sentences like <em>I breaked the window</em> are grammatically invalid, but they still manage to get the meaning accross. We manage to make a meaningful fact tuple i.e <code>({person}, break, window)</code>. And the sentence <em>I breaked the window</em> is just a crude way to imply that exactly that.</p>
<p>LLMs like that of GPT-3 are really good at generating text which looks “realistic”. But they face the problem that DALLE-2 faces with compositionality.</p>
<p>The reasons as to why DALLE-2 cannot consistently make an image of a red box on a blue box might just be the same as to why GPT-3 cannot stuff like logical reasoning (sure it can solve small riddles, but what about longer chains of thought?).</p>
<p>Another important point that was made was that children require far less amount of “training data” when compared to that of LLMs to pick up their language.</p>
<p><em>“To date, nobody, ever, has given a convincing and thorough account of how human children (and human children alone) learn language. To get there, You would probably want a rich theory about how people represent meanings”</em></p>
<p>The way we train LLMs today basically follows this principle: <em>“Let’s throw in as many words as possible into the neural net and make it predict the next one in hopes that this will somehow make it smart.”</em></p>
<p>My personal opinion is that these LLMs are just really really good clever hanses. They are very good at being grammatically fluent, but that does not imply that they’re a good model for the human mind. They are like very good parrots, knowing how to arrange words in the right order, but never to actually understand them.</p>
<p>GPTs do not tell us things about the world, it simply imitates the patterns of human language. It has no form of reasoning built into it.</p>
<h2 id="the-third-wave">The Third wave</h2>
<p>The solution to some of the drawbacks of current deep-learning method might just emerge from something called “Neurosymbolic AI”.</p>
<p><strong>What is neurosymbolic AI?</strong></p>
<p>It is an alternative approach to build systems which not just classify/predict certain variables, but also have some sort of an internal mechanism of logical reasoning. It’s a way to make neural networks “think step by step” so that they generalise with lower amounts of data.</p>
<p><strong>AI cookbook</strong></p>
<p>Gary Marcus proposes that we should build a system that can acquire, represent, and manipulate abstract knowledge.</p>
<p>In order to be actually smart, AGI would require a form of symbolic manipulation of variables in logic.</p>
</body>
</html>
