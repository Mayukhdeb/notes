<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Order From Chaos (Part 2): Diffusion for image synthesis explained in code and a little bit of math</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Order From Chaos (Part 2): Diffusion for image synthesis explained in code and a little bit of math</h1>
</header>
<p><strong>Warning: This post is still being written and is not complete, I just uploaded a draft.</strong></p>
<p>This post is basically what I learned while watching <a href="https://www.youtube.com/watch?v=a4Yfz2FxXiY">this video</a> by DeepFindr.</p>
<p>Diffusion models work by destroying an input gradually until it looks like noise and then recovering the input image from that. The forward process is hardcoded, and the reverse process is trainable.</p>
<p>In the reverse process, the task of the model is to predict the noise that was added in each step to the input image.</p>
<p>We need 3 things for training a diffusion model:</p>
<ol type="1">
<li>A Scheduler that sequentially adds noise</li>
<li>A model that predicts the noise in an image (a U-Net)</li>
<li>A time-step encoding component</li>
</ol>
<h2 id="the-forward-process">The forward process</h2>
{{&lt; math.inline &gt;}}
<p>
In simple words, we iteratively add noise into the image where the amount of noise added per step is dependent on a parameter ()
</p>
<p>{{&lt; /math.inline &gt;}}</p>
<p>In fancy math terms, this is how we perform the markov process:</p>
<p><br /><span class="math display">$$
  q(x_{1:T}|x_0) = \prod_{t=1}^{t=T}q(x_t|x_{t-1})
$$</span><br /></p>
{{&lt; math.inline &gt;}}
<p>
(x_{1:T}) = set of samples where every subsequent item is noisier starting from the orignial image. (x_1) is the input image after adding some noise for the first time (i.e the first step) and (x_T) is the most noisy sample.
</p>
<p>
(<em>{t=1}^{t=T}q(x_t|x</em>{t-1})) is the product of the noise samples for all values of (t) starting from 1 to (T)
</p>
<p>{{&lt; /math.inline &gt;}}</p>
<p><strong>Diving deeper into each noise sample {{&lt; math.inline &gt;}}(q(x_t|x_{t-1})){{&lt;/ math.inline &gt;}}</strong></p>
<p>First, let’s see how it’s defined:</p>
<p><br /><span class="math display">$$
 q(x_t|x_{t-1}) = N(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_tI) 
$$</span><br /></p>
<ul>
<li><p>{{&lt; math.inline &gt;}}</p>
<p>
<p>(_t) determines the variance of the noise to be added in each step into the image.</p>
</p>
<p>{{&lt; /math.inline &gt;}}</p></li>
<li><p>{{&lt; math.inline &gt;}}</p>
<p>
<p>(x_{t-1}) is the previous less noisy image.</p>
</p>
<p>{{&lt; /math.inline &gt;}}</p></li>
<li><p>{{&lt; math.inline &gt;}}</p>
<p>
<p>() scales the mean of the noise to be added. Thus one can say that the mean of our distribution is () multiplied by (x_{t-1}) (for each pixel).</p>
</p>
<p>{{&lt; /math.inline &gt;}}</p></li>
<li><p>{{&lt; math.inline &gt;}}</p>
<p>
<p>(I) is the Identity</p>
</p>
<p>{{&lt; /math.inline &gt;}}</p></li>
</ul>
{{&lt; math.inline &gt;}}
<p>
The sequence of such betas (_1), (_2)… (_t) is known as the variance schedule. They determine how much noise we’d want to add in each of the time steps.
</p>
<p>{{&lt; /math.inline &gt;}}</p>
<p><strong>Diving deeper into {{&lt; math.inline &gt;}}(){{&lt;/ math.inline &gt;}}</strong></p>
<p>Let us imagine for a second that we have an image with a single pixel and then try to understand what then equation above means:</p>
{{&lt; math.inline &gt;}}
<p>
(q(x|x_{t-1})) = the value of the next pixel (q of (x) given (x_{t-1}))
</p>
<p>{{&lt; /math.inline &gt;}}</p>
<center>
<img src = 'https://user-images.githubusercontent.com/53133634/209522028-ad0081b5-c233-4039-8d80-8a399f94c7a3.png' width = "40%"> Image taken from DeepFindr’s video[1] at 8m47s.
</center>
<ul>
<li><p>{{&lt; math.inline &gt;}}</p>
<p>
<p>() is the mean of the dstribution from which we would sample the next pixel</p>
</p>
<p>{{&lt; /math.inline &gt;}}</p></li>
<li><p>{{&lt; math.inline &gt;}}</p>
<p>
<p>() is the variance.</p>
</p>
<p>{{&lt; /math.inline &gt;}}</p></li>
</ul>
<p>{{&lt; math.inline &gt;}} So increasing () would result in the distribution shifting to the left and also becoming more flattened (w i d e r). Kind of like the blue distribution shown below. {{&lt; /math.inline &gt;}}</p>
<center>
<img src = 'https://user-images.githubusercontent.com/53133634/209523096-53d22a91-6c9a-4af7-9c74-b683b39b9749.png' width = "40%"> Image taken from DeepFindr’s video[1] at 9m28s.
</center>
<p>Beta determines how fast we converge towards a mean of zero which is basically a standard gaussian distribution. Beta increases linearly with each time step (from like <code>0.0001</code> to <code>0.02</code> in 200 steps)</p>
<h3 id="speeding-things-up">Speeding things up</h3>
{{&lt; math.inline &gt;}}
<p>
The neat thing about gaussians is that the sum of gaussians is also a gaussian. Which means it’s pretty easy to pre-compute the noisy image at forward time-step (t)
</p>
<p>
Now for convenience, we would make a new variable (_t = 1 - _t). Since beta was being scaled up, alpha would be scaled down on each step. You can think of alpha as the variable which determines how much information is conserved from the previous image in each time step.
</p>
<p>
The nice part is that we can just take the cumulative products of alpha (({_t})) and then we can compute the image at a forward step (t) without having to calculate all the way until step (t-1) first. This way, we can re-define the noise sampling as follows:
</p>
<p>{{&lt; /math.inline &gt;}}</p>
<p><br /><span class="math display">$$
 q(x_t|x_{t_0}) = N(x_t;\sqrt{\bar{\alpha_t}}x_{0}, (1 - \bar{\alpha_t})I) 
$$</span><br /></p>
<p>{{&lt; math.inline &gt;}} Notice how this function is dependend only on (x_0) and not on (x_t) but it computes the noisy pixel value at time step (t). {{&lt; /math.inline &gt;}}</p>
<h3 id="finally-some-code">Finally, some code</h3>
<p>I’ll try to explain things line-by-line:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a><span class="kw">def</span> linear_beta_schedule(timesteps, start<span class="op">=</span><span class="fl">0.0001</span>, end<span class="op">=</span><span class="fl">0.02</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>    <span class="co">## Interpolates between 2 values with a pre-defined number of timesteps. </span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>    <span class="co">## Returns a list of Betas</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a>    <span class="cf">return</span> torch.linspace(start, end, timesteps)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a><span class="kw">def</span> get_index_from_list(vals, t, x_shape):</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot; </span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true"></a><span class="co">    Returns a specific index t of a passed list of values vals</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true"></a><span class="co">    while considering the batch dimension.</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true"></a>    batch_size <span class="op">=</span> t.shape[<span class="dv">0</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true"></a>    out <span class="op">=</span> vals.gather(<span class="op">-</span><span class="dv">1</span>, t.cpu())</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true"></a>    <span class="cf">return</span> out.reshape(batch_size, <span class="op">*</span>((<span class="dv">1</span>,) <span class="op">*</span> (<span class="bu">len</span>(x_shape) <span class="op">-</span> <span class="dv">1</span>))).to(t.device)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true"></a><span class="kw">def</span> forward_diffusion_sample(x_0, t, device<span class="op">=</span><span class="st">&quot;cpu&quot;</span>):</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true"></a>    <span class="co">## takes the input image and the timestep number t as input</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true"></a>    <span class="co">## and returns it&#39;s noisy version at timestep t</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true"></a>    noise <span class="op">=</span> torch.randn_like(x_0)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true"></a>    sqrt_alphas_cumprod_t <span class="op">=</span> get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true"></a>    sqrt_one_minus_alphas_cumprod_t <span class="op">=</span> get_index_from_list(</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true"></a>        sqrt_one_minus_alphas_cumprod, t, x_0.shape</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true"></a>    )</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true"></a>    <span class="co"># mean + variance</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true"></a>    <span class="cf">return</span> sqrt_alphas_cumprod_t.to(device) <span class="op">*</span> x_0.to(device) <span class="op">\</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true"></a>    <span class="op">+</span> sqrt_one_minus_alphas_cumprod_t.to(device) <span class="op">*</span> noise.to(device), noise.to(device)</span></code></pre></div>
<p>The last line in the above snippet is equivalent to:</p>
<p><br /><span class="math display">$$
 x_t = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon
$$</span><br /></p>
<p>{{&lt; math.inline &gt;}} Where () is the noise added into the image. {{&lt; /math.inline &gt;}}</p>
<p>Then, we pre-compute the variables for convenience:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>alphas <span class="op">=</span> <span class="fl">1.</span> <span class="op">-</span> betas</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>alphas_cumprod <span class="op">=</span> torch.cumprod(alphas, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>alphas_cumprod_prev <span class="op">=</span> F.pad(alphas_cumprod[:<span class="op">-</span><span class="dv">1</span>], (<span class="dv">1</span>, <span class="dv">0</span>), value<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>sqrt_recip_alphas <span class="op">=</span> torch.sqrt(<span class="fl">1.0</span> <span class="op">/</span> alphas)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>sqrt_alphas_cumprod <span class="op">=</span> torch.sqrt(alphas_cumprod)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>sqrt_one_minus_alphas_cumprod <span class="op">=</span> torch.sqrt(<span class="fl">1.</span> <span class="op">-</span> alphas_cumprod)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>posterior_variance <span class="op">=</span> betas <span class="op">*</span> (<span class="fl">1.</span> <span class="op">-</span> alphas_cumprod_prev) <span class="op">/</span> (<span class="fl">1.</span> <span class="op">-</span> alphas_cumprod)</span></code></pre></div>
<h2 id="the-neural-network">The Neural network</h2>
<p>The authors proposed to use a a U-Net, which is an image-to-image model. Here the input is the noisy image, and the output is the “noise” predicted by the model.</p>
<p>{{&lt; math.inline &gt;}} So if we have an input (x_t = x0 + ) then the model’s output would ideally be () or something close to it. {{&lt; /math.inline &gt;}}</p>
<h2 id="encoding-time-steps">Encoding time steps</h2>
<p>Note that we also have to let the model know in which timestep the input image is, this is done with the help of positional embeddings.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">class</span> SinusoidalPositionEmbeddings(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>        <span class="va">self</span>.dim <span class="op">=</span> dim</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, time):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a>        device <span class="op">=</span> time.device</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a>        half_dim <span class="op">=</span> <span class="va">self</span>.dim <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a>        embeddings <span class="op">=</span> math.log(<span class="dv">10000</span>) <span class="op">/</span> (half_dim <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a>        embeddings <span class="op">=</span> torch.exp(torch.arange(half_dim, device<span class="op">=</span>device) <span class="op">*</span> <span class="op">-</span>embeddings)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a>        embeddings <span class="op">=</span> time[:, <span class="va">None</span>] <span class="op">*</span> embeddings[<span class="va">None</span>, :]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a>        embeddings <span class="op">=</span> torch.cat((embeddings.sin(), embeddings.cos()), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a>        <span class="cf">return</span> embeddings</span></code></pre></div>
<p><strong>Intuition behind <code>SinusoidalPositionEmbeddings</code></strong></p>
<p>(btw this also applies to transformers in general)</p>
<p>To understand this, first let’s take a look at how binary numbers work. You might be able to notice here that the right-most bit (red) oscillates between 0 and 1 the fastest and the subsequent ones (blue, green, yellow) oscillate in slower and slower rates.</p>
<center>
<img src = "https://user-images.githubusercontent.com/53133634/209769905-70663f7f-aa2c-4ee6-9fd3-590f4245b089.png" width = "50%">
</center>
<p>Now if we use these values as positional encodings, the last 2 bits are good at giving fine-grained relative positions between 2 positions which are close to each other. While the larger bits are good at encoding the position at a larger scale. This way, the model would get both the overall location of each token but would also be able to distinguish between tokens which are very close to each other.</p>
<p>But in real life, binary numbers take up lots of space. So instead we use the something fancier: Sinusoids.</p>
<center>
<img src = "https://user-images.githubusercontent.com/53133634/209771097-1e970289-bfbc-4624-bad6-639f98aac172.png" width = "100%">
</center>
<p>In the figure above, you can see how the first column (left) oscillates the fastest and every column after that one oscillates slower and slower. These are sinusoidal positional embeddings with a depth of 128.</p>
<h2 id="combining-time-step-encoding-with-image-data">Combining time-step encoding with image data</h2>
<p>I will not go into depth about the U-Net architecture itself, because it can be and has been replaced with other models for diffusion. You can just think of it as a model which takes an image as an input and is supposed to predict the noise in the input (which gets subtracted).</p>
<p>The important thing to note here is that we do not concatenate the time embeddings into the data, instead we add it right after passing the input through the first conv layer of a U-Net block.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a><span class="co">    A single building block from the U-Net</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_ch, out_ch, time_emb_dim, up<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a>        <span class="va">self</span>.time_mlp <span class="op">=</span>  nn.Linear(time_emb_dim, out_ch)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a>        <span class="cf">if</span> up:</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a>            <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">2</span><span class="op">*</span>in_ch, out_ch, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a>            <span class="va">self</span>.transform <span class="op">=</span> nn.ConvTranspose2d(out_ch, out_ch, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a>        <span class="cf">else</span>:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a>            <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_ch, out_ch, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a>            <span class="va">self</span>.transform <span class="op">=</span> nn.Conv2d(out_ch, out_ch, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(out_ch, out_ch, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true"></a>        <span class="va">self</span>.bnorm1 <span class="op">=</span> nn.BatchNorm2d(out_ch)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true"></a>        <span class="va">self</span>.bnorm2 <span class="op">=</span> nn.BatchNorm2d(out_ch)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true"></a>        <span class="va">self</span>.relu  <span class="op">=</span> nn.ReLU()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true"></a>        </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, t, ):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true"></a>        <span class="co"># First Conv</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true"></a>        h <span class="op">=</span> <span class="va">self</span>.bnorm1(<span class="va">self</span>.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true"></a>        <span class="co"># Time embedding</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true"></a>        time_emb <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.time_mlp(t))</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true"></a>        <span class="co"># Extend last 2 dimensions</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true"></a>        time_emb <span class="op">=</span> time_emb[(..., ) <span class="op">+</span> (<span class="va">None</span>, ) <span class="op">*</span> <span class="dv">2</span>]</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true"></a>        <span class="co">&#39;&#39;&#39;</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true"></a><span class="co">        This is where we add in the time embedding</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true"></a><span class="co">        combines time step and image information</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true"></a><span class="co">        &#39;&#39;&#39;</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true"></a>        h <span class="op">=</span> h <span class="op">+</span> time_emb</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true"></a>        <span class="co"># Second Conv</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true"></a>        h <span class="op">=</span> <span class="va">self</span>.bnorm2(<span class="va">self</span>.relu(<span class="va">self</span>.conv2(h)))</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true"></a>        <span class="co"># Down or Upsample</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true"></a>        <span class="cf">return</span> <span class="va">self</span>.transform(h)</span></code></pre></div>
<h2 id="loss-function">Loss function</h2>
<p>The loss function is basically the L1 loss on the predicted noise v/s the original noise:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="kw">def</span> get_loss(model, x_0, t):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a><span class="co">    model: U-Net model through which we pass the image</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a><span class="co">    x_0: original input image</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a><span class="co">    t: a specific time step</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a><span class="co">    functions within:</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a><span class="co">        * forward_diffusion_sample: returns the noisy version of the original image and the noise that was last added for the time step t</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a><span class="co">        * model.__call__: takes as input the noisy image and the time step, tries to predict the noise that was last added for the time step t</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a>    x_noisy, noise <span class="op">=</span> forward_diffusion_sample(x_0, t, device)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a>    noise_pred <span class="op">=</span> model(x_noisy, t)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true"></a>    <span class="cf">return</span> F.l1_loss(noise, noise_pred)</span></code></pre></div>
<h2 id="sampling">Sampling</h2>
<p>TODO: explain this</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a><span class="kw">def</span> sample_timestep(x, t):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a><span class="co">    Calls the model to predict the noise in the image and returns </span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a><span class="co">    the denoised image. </span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a><span class="co">    Applies noise to this image, if we are not in the last step yet.</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a>    betas_t <span class="op">=</span> get_index_from_list(betas, t, x.shape)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a>    sqrt_one_minus_alphas_cumprod_t <span class="op">=</span> get_index_from_list(</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true"></a>        sqrt_one_minus_alphas_cumprod, t, x.shape</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true"></a>    )</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true"></a>    sqrt_recip_alphas_t <span class="op">=</span> get_index_from_list(sqrt_recip_alphas, t, x.shape)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true"></a>    </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true"></a>    <span class="co"># Call model (current image - noise prediction)</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true"></a>    model_mean <span class="op">=</span> sqrt_recip_alphas_t <span class="op">*</span> (</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true"></a>        x <span class="op">-</span> betas_t <span class="op">*</span> model(x, t) <span class="op">/</span> sqrt_one_minus_alphas_cumprod_t</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true"></a>    )</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true"></a>    posterior_variance_t <span class="op">=</span> get_index_from_list(posterior_variance, t, x.shape)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true"></a>    </span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true"></a>    <span class="cf">if</span> t <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true"></a>        <span class="cf">return</span> model_mean</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true"></a>    <span class="cf">else</span>:</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true"></a>        noise <span class="op">=</span> torch.randn_like(x)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true"></a>        <span class="cf">return</span> model_mean <span class="op">+</span> torch.sqrt(posterior_variance_t) <span class="op">*</span> noise </span></code></pre></div>
<h2 id="references">References</h2>
<p>[1] - <a href="https://www.youtube.com/watch?v=a4Yfz2FxXiY">DeepFindr’s video</a></p>
</body>
</html>
