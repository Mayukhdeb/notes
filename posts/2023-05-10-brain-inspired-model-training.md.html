<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>BIMT: Brain Inspired Modular Training</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">BIMT: Brain Inspired Modular Training</h1>
</header>
<p>When compared to artificial neural networks, the human brain is a lot more modular. The authors believe that this is because the loss function of ANN regularizers like weight decay are not dependent on the permutations of neurons on each layer.</p>
<p>The cost of connecting 2 biological neurons which are far apart is much more than when theyâ€™re closer together. But this is not the case for ANNs.</p>
<p>In order to impose a similar phenomenon to that of brains, the authors propose the following steps:</p>
<ol type="1">
<li>Embed every neuron in a model into a 2D space where x = neuron index and y layer index</li>
<li>Augment the loss function with a cost proportional to the length of each neuron connection times the absolute value of the connection weight</li>
</ol>
<p>Number 2 aims to keep neurons that need to communicate as close together as possible</p>
<!-- 
I think we can also apply this method to LLM training to see if we get simple circuits as a emergent behaviour during training.
 -->
<h3 id="weight-layers-vs-neuron-layers">Weight layers vs neuron layers</h3>
<p>There is a difference between weight layers and neuron layers.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>    <span class="co">## ...there can be more layers over here</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>    <span class="co"># neuron layer with 10 neurons</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">7</span>),              <span class="co">## weight layer</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>    <span class="co"># neuron layer with 7 neurons</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a>    nn.Linear(<span class="dv">7</span>, <span class="dv">5</span>),               <span class="co">## weight layer</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a>    <span class="co"># neuron layer with 5 neurons</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a>    nn.Linear(<span class="dv">5</span>,<span class="dv">3</span>)                 <span class="co">## weight layer</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a>    <span class="co"># neuron layer with 3 neurons</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true"></a>)</span></code></pre></div>
<ul>
<li><p><strong>Weight layers</strong>: actual <code>nn.Linear</code> modules. Where <code>num_inputs</code> and <code>num_outputs</code> are the number of neurons incoming and outgoing from a single layer. A model with <code>L</code> linear layers -&gt; there are <code>L</code> weight layers</p></li>
<li><p><strong>Neuron Layers</strong>: The neuron layers are basically the outputs of each layer.</p></li>
</ul>
<h3 id="representing-weights-in-2d-space">Representing weights in 2D space</h3>
<p>In order to represent distance in between each neuron in each layer. We arrange all neurons in a 2D plane such that <code>x</code> refers to the neuron index and <code>y</code> refers to the layer index.</p>
<ul>
<li>Every neuron in the same layer would have the same y co-ordinate, but different x co-ordinate. Neurons are seperated from each other uniformly horizontally.</li>
<li>Layers in different neurons have different y co-ordinates.</li>
</ul>
<h2 id="defining-the-spatial-relation-in-between-neurons">Defining the spatial relation in between neurons</h2>
<p>In between the neuron layers, lies a weight layer with weights <code>W</code>.</p>
<p>This layer contains a matrix of size <code>(num_input_neurons, num_output_neurons)</code>.</p>
<p>Let us imagine a simple pytorch model:</p>
<p>The weight that connects neuron index 4 of first neuron layer to the neuron index 2 of the 2nd neuron layer is: <code>W[4, 2]</code></p>
<p>Given 2 neurons <code>n1: (x1, y1)</code> and <code>n2: (x2, y2)</code> where <code>(x,y)</code> is the position of the neurons in 2D space, the distance between them can either be the L1 distance or the L2 Norm.</p>
<ul>
<li>L1: <code>abs(x1 - x2) + abs(y1 - y2)</code></li>
<li>L2: <code>(x1 - x2)**2 + (y1 - y2)**2</code></li>
</ul>
<h2 id="localization-to-encourage-locality">Localization to encourage locality</h2>
<p>WIP</p>
</body>
</html>
