<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</h1>
</header>
<h2 id="what-are-they-doing">What are they doing?</h2>
<p>They found a way to help a robot make a “map” of the world around it in terms of multimodal scene encodings. Then they store these multimodal scene encodings and their respective labels (“chair”) on a database which is differentiable and is also easily searchable.</p>
<h2 id="how-are-they-doing-it">How are they doing it?</h2>
<ol type="1">
<li>In order to collect data, they used an RGB-D from which the following data was collected:</li>
</ol>
<ul>
<li>RGB-D data</li>
<li>bounding boxes from pre-trained object detector
<ul>
<li>the labels of the bounding boxes are fed into a BERT</li>
<li>the bounding boxes on the image are used to crop it and feed each crop into CLIP and store the CLIP encoding</li>
</ul></li>
<li>Spatial location (x, y, z)</li>
</ul>
<p>They used an iphone 13 pro with a LiDAR sensor for depth image sequences.</p>
<ol start="2" type="1">
<li>Robot execution pipeline <a href="https://github.com/notmahi/clip-fields/blob/main/demo/4%20-%20test%20model.ipynb">as seen here</a></li>
</ol>
<ul>
<li>When the robot gets a new text query, first we feed it through the SBERT (sentence BERT) and then also through CLIP</li>
<li>SBERT gives us the semantic part of the query</li>
<li>CLIP gives us the vision-aligned encoding</li>
</ul>
<p>The robot then looks through it’s database and finds which data-point is such that there is max similarity between the semantic representation and the visually aligned encoding.</p>
<p>Let us walk through some part of the code that the author wrote. Read the comments given below along with the code:</p>
<p>This is where they generate encodings from a given text query from the model.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">def</span> calculate_clip_and_st_embeddings_for_queries(queries):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>    <span class="co">## queries is a string which first gets tokenized</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>    all_clip_queries <span class="op">=</span> clip.tokenize(queries)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>        <span class="co">## encode text with CLIP to get &quot;visually aligned&quot; encoding</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>        all_clip_tokens <span class="op">=</span> model.encode_text(all_clip_queries.to(DEVICE)).<span class="bu">float</span>()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a>        <span class="co">## normalize encodings, dont exactly know why. Maybe they just want the directional information, kinda like a unit vector.</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a>        all_clip_tokens <span class="op">=</span> F.normalize(all_clip_tokens, p<span class="op">=</span><span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a>        <span class="co">## encode text with SBERT to get a nice text encoding (idk semantic?)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true"></a>        all_st_tokens <span class="op">=</span> torch.from_numpy(sentence_model.encode(queries))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true"></a>        <span class="co">## normalize encodings, dont exactly know why. Maybe they just want the directional information, kinda like a unit vector.</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true"></a>        all_st_tokens <span class="op">=</span> F.normalize(all_st_tokens, p<span class="op">=</span><span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>).to(DEVICE)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true"></a>    <span class="cf">return</span> all_clip_tokens, all_st_tokens</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true"></a>query <span class="op">=</span> <span class="st">&quot;Warm up my lunch&quot;</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true"></a>clip_text_tokens, st_text_tokens <span class="op">=</span> calculate_clip_and_st_embeddings_for_queries([query])</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;query =&quot;</span>, query)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;tokens shape =&quot;</span>, clip_text_tokens.shape)</span></code></pre></div>
<p>This is where they use the multimodal encodings from the query to perform a search in the robot’s memory:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="kw">def</span> find_alignment_over_model(label_model, queries, dataloader, visual<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>    <span class="co">## This is the fn that we just discussed about</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>    clip_text_tokens, st_text_tokens <span class="op">=</span> calculate_clip_and_st_embeddings_for_queries(queries)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>    <span class="co"># We give different weights to visual and semantic alignment </span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>    <span class="co"># for different types of queries</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>    <span class="cf">if</span> visual:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a>        vision_weight <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>        text_weight <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a>    <span class="cf">else</span>:</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true"></a>        vision_weight <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true"></a>        text_weight <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true"></a>    point_opacity <span class="op">=</span> []</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true"></a>    <span class="co">## iterate over the entire dataset (wow thats gonna be computationally expensive)</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true"></a>        <span class="cf">for</span> data <span class="kw">in</span> tqdm.tqdm(dataloader, total<span class="op">=</span><span class="bu">len</span>(dataloader)):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true"></a>            <span class="co"># Find alignmnents with the vectors</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true"></a>            <span class="co">## for a single dataset instance, we generate the semantic and the visual encodings and normalise them</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true"></a>            predicted_label_latents, predicted_image_latents <span class="op">=</span> label_model(data.to(DEVICE))</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true"></a>            data_text_tokens <span class="op">=</span> F.normalize(predicted_label_latents, p<span class="op">=</span><span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>).to(DEVICE)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true"></a>            data_visual_tokens <span class="op">=</span> F.normalize(predicted_image_latents, p<span class="op">=</span><span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>).to(DEVICE)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true"></a>            <span class="co">## note that similarity = dot product</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true"></a>            <span class="co">## calculate similarity between query text encoding and dataset instance label encoding</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true"></a>            text_alignment <span class="op">=</span> data_text_tokens <span class="op">@</span> st_text_tokens.T</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true"></a>            <span class="co">## calculate similarity between query visual encoding and dataset instance CLIP encoding </span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true"></a>            visual_alignment <span class="op">=</span> data_visual_tokens <span class="op">@</span> clip_text_tokens.T</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true"></a>            <span class="co">## some sort of a weighted sum to prioritize one over the other</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true"></a>            total_alignment <span class="op">=</span> (text_weight <span class="op">*</span> text_alignment) <span class="op">+</span> (vision_weight <span class="op">*</span> visual_alignment)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true"></a>            total_alignment <span class="op">/=</span> (text_weight <span class="op">+</span> vision_weight)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true"></a>            <span class="co">## append all of these weighted similarity scores into a list</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true"></a>            point_opacity.append(total_alignment)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true"></a>    point_opacity <span class="op">=</span> torch.cat(point_opacity).T</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true"></a>    <span class="bu">print</span>(point_opacity.shape)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true"></a>    <span class="cf">return</span> point_opacity</span></code></pre></div>
<p>Dot product is the same as taking the cosine similarity between the query and the point embeddings.</p>
<h2 id="my-opinionsfurther-discussion">My opinions/further discussion</h2>
<ul>
<li><p>What do we do for super large spaces with lots of stuff? The dataset would become very large and then it would probably not be practical to run over the entire dataset in real-time. A possible solution (given that we have enough storage) is to pre-compute the dataset embeddings and then query them when required for calculating the similarity score. Problem is that even this will take up tons of storage. Probably makes sense to keep the big stuff “on the cloud” and send the queries from the humans via an API to the servers first and then return the results to the robot.</p></li>
<li><p>Can we then also teach this robot to “look for my car keys”? Given that the car keys are beyond the “dataset” of the robot. We can ask it to keep searching for objects in the home and then when it gets a high similarity score for “car keys” for a given weighted query encoding then we return the result to the user saying that “this might be what you’re looking for.”</p></li>
<li><p>Okay so what’s a cheap way to replicate this project? I think we can do this in minecraft. First we would need 3D data for navigations/spatial information. It might not be possible at first to also get a custom minecraft object detector model. So we can just set a constant bounding box and generate CLIP embeddings for that accordingly.</p></li>
<li><p>Another thing that we can try in minecraft is that we can also get a list of objects like “tree” and “chicken” and then in order to search for a tree we just look for the query whose CLIP embedding had the cossim to “tree”. Might be a good idea to do this search in the multimodal embedding space of MAGMA</p></li>
</ul>
</body>
</html>
